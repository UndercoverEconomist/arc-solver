#!/usr/bin/env python3
"""
Test script for ARC prompt generation and model execution.

This script loads an ARC task from the training challenges dataset,
generates a prompt using build_arc_prompt, and runs it with a selected model.
"""

import json
import os
import random
import re
import subprocess
import sys
import io
import threading
import time
import argparse
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

import numpy as np
import requests
from dotenv import load_dotenv

from model_configs import MODEL_CONFIGS, is_known_model, estimate_cost
from prompts import (build_arc_prompt, build_arc_reflection_prompt, build_code_repair_prompt,
                     create_continuation_prompt, create_json_regeneration_prompt, 
                     create_json_repair_prompt, create_code_repair_prompt as create_enhanced_code_repair_prompt)
from utils import sanitize_task, calculate_results

# ============================================================================
# CONFIGURATION VARIABLES - Modify these as needed
# ============================================================================

# Task selection: Set to specific index (0-399), task ID string, or None for random selection
TASK_INDEX = None  # Example: 0, 42, 150, "00d62c1b", or None

# Number of tasks to run (if TASK_INDEX is None, will run this many random tasks)
NUMBER_OF_TASKS = 1  # Set to 1 for single task, or higher for multiple tasks

# Total task runs including retries (separate from NUMBER_OF_TASKS)
TOTAL_TASK_RUNS = 3  # Total number of task runs including initial attempts and retries

# Model selection: Choose from available models in model_configs.py
MODEL = "gemini-2.5-flash-lite-preview-06-17"  # The other is "qwen2.5:32b"

# Path to the ARC challenges and solutions JSON files
YEAR = 2024
if YEAR == 2024:
    TRAINING_CHALLENGES_PATH = "data/arc-2024/arc-agi_training_challenges.json"
    TRAINING_SOLUTIONS_PATH = "data/arc-2024/arc-agi_training_solutions.json"
    EVALUATION_CHALLENGES_PATH = "data/arc-2024/arc-agi_evaluation_challenges.json"
    EVALUATION_SOLUTIONS_PATH = "data/arc-2024/arc-agi_evaluation_solutions.json"
else:
    TRAINING_CHALLENGES_PATH = "data/arc-2025/arc-agi_training_challenges.json"
    TRAINING_SOLUTIONS_PATH = "data/arc-2025/arc-agi_training_solutions.json"
    EVALUATION_CHALLENGES_PATH = "data/arc-2025/arc-agi_evaluation_challenges.json"
    EVALUATION_SOLUTIONS_PATH = "data/arc-2025/arc-agi_evaluation_solutions.json"

# Random seed for reproducible task selection (set to None for random)
RANDOM_SEED = None  # Example: 42, 123, or None

# Model parameters (for Ollama models)
TEMPERATURE = 0.6  # Controls randomness (0.0 = deterministic, 1.0 = very random)
REFLECTION_TEMPERATURE = 0.3  # Lower temperature for reflection to follow instructions better
NUM_PREDICT = -1  # Maximum number of tokens to generate (-1 = no limit)

# Task sanitization: Whether to use sanitized task data and ID
SANITIZE_TASK = False  # Set to False to use original task representation

# Sanitized ID: Whether to use sanitized task ID (only applies when SANITIZE_TASK is True)
SANITIZE_ID = True  # Set to False to keep original task ID even when using sanitized data

# Execution settings
PARALLEL = False  # Set to True to run tasks in parallel (output buffered and displayed per task completion)
PRINT_INPUT = True  # Set to False to hide prompts sent to the language model
PRINT_OUTPUT = True  # Set to False to hide model responses and detailed output
PRINT_JSON = True  # Set to True to print the transformation JSON generated by the model

# Enhanced processing settings
USE_SMART_ROUTER = True  # Set to True to use smart router for response enhancement with any model
MAX_REFLECTIONS = 5  # Maximum number of reflection cycles for complete enhancement

# Task retry settings
TASK_RETRIES = 2  # Number of times to retry a task if it fails to generate valid JSON (0 = no retries)

# Continue run settings
EXISTING_OUTPUT = None  # Set to path of existing output folder to continue a previous run

# ANSI color codes for terminal output
BLUE = '\033[94m'
GREEN = '\033[92m'
RED = '\033[91m'
RESET = '\033[0m'

def parse_arguments():
    """Parse command line arguments with defaults from ALL_CAPS variables."""
    parser = argparse.ArgumentParser(description="ARC Test Script")
    
    parser.add_argument("--task-index", type=str, default=TASK_INDEX, 
                        help=f"Index (0-399) or task ID string to run (default: {TASK_INDEX})")
    parser.add_argument("--number-of-tasks", type=int, default=NUMBER_OF_TASKS,
                        help=f"Number of tasks to run (default: {NUMBER_OF_TASKS})")
    parser.add_argument("--total-task-runs", type=int, default=TOTAL_TASK_RUNS,
                        help=f"Total task runs including retries (default: {TOTAL_TASK_RUNS})")
    parser.add_argument("--model", type=str, default=MODEL,
                        help=f"Model to use for inference (default: {MODEL})")
    parser.add_argument("--training-challenges-path", type=str, default=TRAINING_CHALLENGES_PATH,
                        help=f"Path to training challenges JSON file (default: {TRAINING_CHALLENGES_PATH})")
    parser.add_argument("--training-solutions-path", type=str, default=TRAINING_SOLUTIONS_PATH,
                        help=f"Path to training solutions JSON file (default: {TRAINING_SOLUTIONS_PATH})")
    parser.add_argument("--evaluation-challenges-path", type=str, default=EVALUATION_CHALLENGES_PATH,
                        help=f"Path to evaluation challenges JSON file (default: {EVALUATION_CHALLENGES_PATH})")
    parser.add_argument("--evaluation-solutions-path", type=str, default=EVALUATION_SOLUTIONS_PATH,
                        help=f"Path to evaluation solutions JSON file (default: {EVALUATION_SOLUTIONS_PATH})")
    parser.add_argument("--random-seed", type=int, default=RANDOM_SEED,
                        help=f"Random seed for task selection (default: {RANDOM_SEED})")
    parser.add_argument("--temperature", type=float, default=TEMPERATURE,
                        help=f"Temperature for model generation (default: {TEMPERATURE})")
    parser.add_argument("--num-predict", type=int, default=NUM_PREDICT,
                        help=f"Number of tokens to predict (-1 for default) (default: {NUM_PREDICT})")
    parser.add_argument("--sanitize-task", action="store_true", default=SANITIZE_TASK,
                        help=f"Use sanitized task representation (default: {SANITIZE_TASK})")
    parser.add_argument("--sanitize-id", action="store_true", default=SANITIZE_ID,
                        help=f"Use sanitized task ID (default: {SANITIZE_ID})")
    parser.add_argument("--parallel", action="store_true", default=PARALLEL,
                        help=f"Run tasks in parallel (default: {PARALLEL})")
    parser.add_argument("--print-input", action="store_true", default=PRINT_INPUT,
                        help=f"Print input prompts (default: {PRINT_INPUT})")
    parser.add_argument("--print-output", action="store_true", default=PRINT_OUTPUT,
                        help=f"Print model outputs (default: {PRINT_OUTPUT})")
    parser.add_argument("--print-json", action="store_true", default=PRINT_JSON,
                        help=f"Print transformation JSON generated by model (default: {PRINT_JSON})")
    parser.add_argument("--use-smart-router", action="store_true", default=USE_SMART_ROUTER,
                        help=f"Use smart router for response enhancement with any model (default: {USE_SMART_ROUTER})")
    parser.add_argument("--max-reflections", type=int, default=MAX_REFLECTIONS,
                        help=f"Maximum number of reflection cycles for complete enhancement (default: {MAX_REFLECTIONS})")
    parser.add_argument("--task-retries", type=int, default=TASK_RETRIES,
                        help=f"Number of times to retry a task if it fails to generate valid JSON (default: {TASK_RETRIES})")
    parser.add_argument("--existing-output", type=str, default=EXISTING_OUTPUT,
                        help=f"Path to existing output folder to continue a previous run (default: {EXISTING_OUTPUT})")
    
    return parser.parse_args()

# ============================================================================
# HELPERS
# ============================================================================

def format_json_with_compact_arrays(obj, indent=2):
    """Format JSON with compact representation of 2D arrays/grids."""
    
    def is_2d_number_grid(grid):
        """Check if this is a 2D grid of numbers."""
        return (isinstance(grid, list) and grid and 
                isinstance(grid[0], list) and 
                all(isinstance(row, list) for row in grid) and
                all(all(isinstance(cell, (int, float)) for cell in row) for row in grid))
    
    def process_object(obj):
        """Recursively process the object to identify grids."""
        if isinstance(obj, dict):
            result = {}
            for key, value in obj.items():
                result[key] = process_object(value)
            return result
        elif isinstance(obj, list):
            # Check if this looks like a 2D grid
            if (is_2d_number_grid(obj) and len(obj) <= 20 and 
                all(len(row) <= 20 for row in obj)):
                # Mark this as a grid for special formatting
                return {"__GRID__": obj}
            else:
                return [process_object(item) for item in obj]
        else:
            return obj
    
    # Process the object to mark grids
    processed_obj = process_object(obj)
    
    # Convert to JSON string with normal formatting first
    json_str = json.dumps(processed_obj, indent=indent, ensure_ascii=False)
    
    # Post-process to format marked grids compactly
    import re
    
    def format_grid_match(match):
        """Format a matched grid compactly."""
        grid_json = match.group(1)
        try:
            grid = json.loads(grid_json)
            # Format as compact rows
            rows = []
            for row in grid:
                row_str = '[' + ', '.join(str(cell) for cell in row) + ']'
                rows.append(row_str)
            return '[' + ', '.join(rows) + ']'
        except:
            return grid_json  # Fallback if parsing fails
    
    # Replace grid markers with compact formatting
    json_str = re.sub(r'{\s*"__GRID__":\s*(\[(?:\s*\[[\d\s,]*\]\s*,?\s*)*\])\s*}', 
                      format_grid_match, json_str, flags=re.MULTILINE | re.DOTALL)
    
    return json_str


def print_prompt_in_blue(prompt, print_input=False):
    """Print the prompt in blue color."""
    if print_input:
        print(f"\n{BLUE}{'='*60}")
        print("PROMPT SENT TO MODEL:")
        print(f"{'='*60}")
        print(prompt)
        print(f"{'='*60}{RESET}")
    else:
        print(f"{BLUE}[Prompt sent to model - hidden]{RESET}")


def count_tokens_simple(text):
    """Simple token counting approximation (words * 1.3 for rough estimate)."""
    if not text:
        return 0
    # Simple approximation: split by whitespace and multiply by 1.3
    words = len(text.split())
    return int(words * 1.3)


def grid_to_string_lines(grid):
    """Convert grid to array of line-separated strings."""
    if not grid:
        return []
    return [''.join(map(str, row)) for row in grid]


def is_valid_prediction(predicted):
    """Check if prediction is a valid 2D grid."""
    if not predicted:
        return False
    if not isinstance(predicted, list):
        return False
    if not all(isinstance(row, list) for row in predicted):
        return False
    if not predicted:
        return False
    row_length = len(predicted[0])
    return all(len(row) == row_length for row in predicted)


def calculate_grid_iou(predicted, expected):
    """Calculate intersection over union of grid dimensions."""
    if not predicted or not expected:
        return 0.0
    
    pred_h, pred_w = len(predicted), len(predicted[0]) if predicted else 0
    exp_h, exp_w = len(expected), len(expected[0]) if expected else 0
    
    if pred_h == 0 or pred_w == 0 or exp_h == 0 or exp_w == 0:
        return 0.0
    
    # Calculate intersection and union of dimensions
    intersection_h = min(pred_h, exp_h)
    intersection_w = min(pred_w, exp_w)
    union_h = max(pred_h, exp_h)
    union_w = max(pred_w, exp_w)
    
    intersection_area = intersection_h * intersection_w
    union_area = union_h * union_w
    
    return (intersection_area / union_area) * 100.0 if union_area > 0 else 0.0


def print_token_count_in_blue(response):
    """Print the token count in blue color."""
    if response:
        token_count = count_tokens_simple(response)
        print(f"\n{BLUE}{'='*60}")
        print(f"GENERATED TOKENS: {token_count:,}")
        print(f"{'='*60}{RESET}")
    else:
        print(f"\n{BLUE}{'='*60}")
        print("GENERATED TOKENS: 0 (no response)")
        print(f"{'='*60}{RESET}")


def extract_json_from_response(response):
    """Extract JSON from response text, looking for ```json ``` blocks."""
    if not response:
        return None
    
    # Look for ```json ... ``` blocks
    import re
    json_pattern = r'```json\s*\n(.*?)\n```'
    matches = re.findall(json_pattern, response, re.DOTALL)
    
    if not matches:
        return None
    
    # Try to parse the first JSON block found
    try:
        json_text = matches[0].strip()
        return json.loads(json_text)
    except json.JSONDecodeError as e:
        print(f"{RED}Error parsing JSON: {e}{RESET}")
        return None


def validate_json_structure(data):
    """Validate that the JSON has the expected structure."""
    if not isinstance(data, dict):
        return False
    
    if "python_code" not in data:
        return False
    
    if "step_by_step_transformations" not in data:
        return False
    
    steps = data["step_by_step_transformations"]
    if not isinstance(steps, list):
        return False
    
    for step in steps:
        if not isinstance(step, dict):
            return False
        required_fields = ["step_number", "description", "pseudo_code"]
        if not all(field in step for field in required_fields):
            return False
    
    return True


def execute_transformation_code(code_lines, input_grid, helper_functions=None):
    """Execute the transformation code on an input grid, optionally with helper functions."""
    try:
        # Start with helper functions if provided
        all_code_lines = []
        if helper_functions:
            if isinstance(helper_functions, list):
                all_code_lines.extend(helper_functions)
            else:
                all_code_lines.append(helper_functions)
            all_code_lines.append("")  # Add blank line for separation
        
        # Add the main transformation code
        if isinstance(code_lines, list):
            all_code_lines.extend(code_lines)
        else:
            all_code_lines.append(code_lines)
        
        # Join all code lines into a single script
        code = '\n'.join(all_code_lines)
        
        # Create a single namespace for execution
        namespace = {
            "__builtins__": __builtins__, 
            "np": __import__("numpy"),
            "input_grid": input_grid
        }
        
        # Execute the code in the single namespace
        exec(code, namespace)
        
        # Look for the transform function and call it
        if 'transform' in namespace:
            transform_func = namespace['transform']
            result = transform_func(input_grid)
            return result, None  # Success: return result and no error
        else:
            error_msg = "No 'transform' function found in generated code"
            print(f"{RED}Error: {error_msg}{RESET}")
            return None, error_msg
            
    except Exception as e:
        error_msg = f"Error executing transformation code: {str(e)}"
        print(f"{RED}{error_msg}{RESET}")
        return None, error_msg


def calculate_grid_overlap(predicted, expected):
    """Calculate percentage overlap between two grids."""
    if not predicted or not expected:
        return 0.0
    
    if len(predicted) != len(expected):
        return 0.0
    
    total_cells = 0
    matching_cells = 0
    
    for i, (pred_row, exp_row) in enumerate(zip(predicted, expected)):
        if len(pred_row) != len(exp_row):
            return 0.0
        
        for j, (pred_cell, exp_cell) in enumerate(zip(pred_row, exp_row)):
            total_cells += 1
            if pred_cell == exp_cell:
                matching_cells += 1
    
    return (matching_cells / total_cells) * 100.0 if total_cells > 0 else 0.0


def is_task_completed_successfully(result):
    """Check if a task completed successfully (all training examples correct)."""
    if not result or not result.get('trains'):
        return False
    
    # Check if all training examples were correct
    for train_result in result['trains']:
        if not train_result.get('correct', False):
            return False
    
    return True


def test_transformations_on_task(json_data, task_data):
    """Test the transformations from JSON on all training examples in the task."""
    if not validate_json_structure(json_data):
        print(f"{RED}Invalid JSON structure{RESET}")
        return False
    
    # Get the main python_code from the top level
    python_code = json_data["python_code"]
    
    # Get helper functions from JSON
    helper_functions = json_data.get("helper_python_functions", [])
    
    training_examples = task_data.get("train", [])
    if not training_examples:
        print(f"{RED}No training examples found in task{RESET}")
        return False
    
    print(f"\n{GREEN}Testing transformation on {len(training_examples)} training examples...{RESET}")
    
    all_successful = True
    total_overlap = 0.0
    
    for i, example in enumerate(training_examples):
        input_grid = example["input"]
        expected_output = example["output"]
        
        print(f"\nTesting example {i+1}:")
        
        # Execute transformation
        predicted_output, error = execute_transformation_code(python_code, input_grid, helper_functions)
        
        if predicted_output is None:
            print(f"{RED}Failed to execute transformation on example {i+1}{RESET}")
            all_successful = False
            # Store error details for potential refinement
            if not hasattr(test_transformations_on_task, 'first_error'):
                test_transformations_on_task.first_error = {
                    'error': error,
                    'example_idx': i,
                    'input_grid': input_grid,
                    'expected_output': expected_output
                }
            continue
        
        # Calculate overlap
        overlap = calculate_grid_overlap(predicted_output, expected_output)
        total_overlap += overlap
        
        print(f"  Overlap: {overlap:.1f}%")
        if overlap == 100.0:
            print(f"  {GREEN}âœ“ Perfect match!{RESET}")
        elif overlap >= 90.0:
            print(f"  {GREEN}âœ“ Very close match{RESET}")
        elif overlap >= 70.0:
            print(f"  {BLUE}~ Good match{RESET}")
        else:
            print(f"  {RED}âœ— Poor match{RESET}")
    
    if all_successful:
        avg_overlap = total_overlap / len(training_examples)
        print(f"\n{GREEN}Successfully transformed all examples!{RESET}")
        print(f"{GREEN}Average overlap: {avg_overlap:.1f}%{RESET}")
    else:
        print(f"\n{RED}Some transformations failed{RESET}")
    
    return all_successful


def test_on_test_examples(json_data, task_data, task_id, solutions_data):
    """Test the transformations on test examples and compare with solutions."""
    if not validate_json_structure(json_data):
        print(f"{RED}Invalid JSON structure{RESET}")
        return False
    
    # Get the main python_code from the top level
    python_code = json_data["python_code"]
    
    # Get helper functions from JSON
    helper_functions = json_data.get("helper_python_functions", [])
    
    test_examples = task_data.get("test", [])
    if not test_examples:
        print(f"{RED}No test examples found in task{RESET}")
        return False
    
    # Get solutions for this task
    task_solutions = solutions_data.get(task_id, [])
    if not task_solutions:
        print(f"{RED}No solutions found for task {task_id}{RESET}")
        return False
    
    if len(test_examples) != len(task_solutions):
        print(f"{RED}Mismatch: {len(test_examples)} test examples but {len(task_solutions)} solutions{RESET}")
        return False
    
    print(f"\n{GREEN}Testing on {len(test_examples)} test examples...{RESET}")
    
    all_successful = True
    total_overlap = 0.0
    
    for i, (test_example, expected_output) in enumerate(zip(test_examples, task_solutions)):
        input_grid = test_example["input"]
        
        print(f"\nTesting test example {i+1}:")
        
        # Execute transformation
        predicted_output, error = execute_transformation_code(python_code, input_grid, helper_functions)
        
        if predicted_output is None:
            print(f"{RED}Failed to execute transformation on test example {i+1}{RESET}")
            all_successful = False
            # Store error details for potential refinement
            if not hasattr(test_on_test_examples, 'first_error'):
                test_on_test_examples.first_error = {
                    'error': error,
                    'example_idx': i,
                    'input_grid': input_grid,
                    'expected_output': expected_output
                }
            continue
        
        # Calculate overlap
        overlap = calculate_grid_overlap(predicted_output, expected_output)
        total_overlap += overlap
        
        print(f"  Overlap: {overlap:.1f}%")
        if overlap == 100.0:
            print(f"  {GREEN}âœ“ Perfect match!{RESET}")
        elif overlap >= 90.0:
            print(f"  {GREEN}âœ“ Very close match{RESET}")
        elif overlap >= 70.0:
            print(f"  {BLUE}~ Good match{RESET}")
        else:
            print(f"  {RED}âœ— Poor match{RESET}")
    
    if all_successful:
        avg_overlap = total_overlap / len(test_examples)
        print(f"\n{GREEN}Successfully transformed all test examples!{RESET}")
        print(f"{GREEN}Average test overlap: {avg_overlap:.1f}%{RESET}")
    else:
        print(f"\n{RED}Some test transformations failed{RESET}")
    
    return all_successful


def create_error_refinement_prompt(original_json, error_details, failed_example_idx, input_grid, expected_output):
    """Create a follow-up prompt to refine the JSON based on errors."""
    
    prompt = f"""The previous transformation code failed with the following error:

ERROR DETAILS:
{error_details}

FAILED ON EXAMPLE {failed_example_idx + 1}:
Input grid:
{json.dumps(input_grid, indent=2)}

Expected output:
{json.dumps(expected_output, indent=2)}

ORIGINAL JSON RESPONSE:
```json
{json.dumps(original_json, indent=2)}
```

Please analyze the error and provide a CORRECTED JSON response that fixes the issue. The corrected code should:

1. Handle the specific error that occurred
2. Work correctly on the failed example
3. Maintain the same structure with "step_by_step_transformations"
4. Include working Python code that can execute without errors

Please provide the complete corrected JSON in the exact required format:

```json
{{
  # Python compatible code that describes
  # any helper functions needed to implement the rule.
  # Each rule will run this code before applying the transformation code.
  "helper_python_functions": [
    "...",
  ],
  "step_by_step_transformations": [{{
      "step_number": 1,
      "description": [
        "...",
      ], # Describe the transformation conceptually
      "pseudo_code": [
      ],
  }},
  "python_code": [
    "def transform(grid):",
    "    # Complete transformation implementation",
    "    # This function must be fully executable on its own",
    "    # and return a complete transformed grid",
    "    return processed_grid"
  ]
}}
```"""

    return prompt


def ask_model_for_refinement(model, provider, original_json, error_details, failed_example_idx, input_grid, expected_output, print_input=False):
    """Ask the model to refine the JSON based on the error."""
    
    print(f"\n{BLUE}{'='*60}")
    print("REQUESTING MODEL REFINEMENT...")
    print(f"{'='*60}{RESET}")
    
    refinement_prompt = create_error_refinement_prompt(
        original_json, error_details, failed_example_idx, input_grid, expected_output
    )
    
    print_prompt_in_blue(refinement_prompt, print_input)
    
    # Call the appropriate model function
    if model == "llama3.1" or provider == "ollama":
        response = run_with_ollama(refinement_prompt, model=model)
    elif provider == "google" or provider == "learnlm":
        response = run_with_gemini(refinement_prompt, model)
    else:
        response = run_with_other_model(refinement_prompt, model)
    
    return response


def process_training_examples(task_data, extracted_json):
    """Process training examples and return results in the new format."""
    training_examples = task_data.get("train", [])
    train_results = []
    
    if not validate_json_structure(extracted_json):
        # If JSON is invalid, mark all training examples as failed
        for i, example in enumerate(training_examples):
            train_entry = {
                'input': grid_to_string_lines(example["input"]),
                'output': grid_to_string_lines(example["output"]),
                'valid_predict': False,
                'predict': [],
                'iou': 0.0,
                'overlap': 0.0,
                'correct': False,
                'transformation_succeed': [False]
            }
            train_results.append(train_entry)
        return train_results
    
    steps = extracted_json["step_by_step_transformations"]
    if not steps:
        # No steps found, mark all as failed
        for i, example in enumerate(training_examples):
            train_entry = {
                'input': grid_to_string_lines(example["input"]),
                'output': grid_to_string_lines(example["output"]),
                'valid_predict': False,
                'predict': [],
                'iou': 0.0,
                'overlap': 0.0,
                'correct': False,
                'transformation_succeed': [False]
            }
            train_results.append(train_entry)
        return train_results
    
    # Get helper functions from JSON
    helper_functions = extracted_json.get("helper_python_functions", [])
    
    # Use the main python_code from the top level for predictions
    python_code = extracted_json.get("python_code", [])
    
    for i, example in enumerate(training_examples):
        input_grid = example["input"]
        expected_output = example["output"]
        
        predicted_output, error = execute_transformation_code(python_code, input_grid, helper_functions)
        
        valid_predict = predicted_output is not None
        if valid_predict:
            iou = calculate_grid_iou(predicted_output, expected_output)
            overlap = calculate_grid_overlap(predicted_output, expected_output)
            correct = (iou == 100.0 and overlap == 100.0)
            predict_lines = grid_to_string_lines(predicted_output)
        else:
            iou = 0.0
            overlap = 0.0
            correct = False
            predict_lines = []
        
        # With the new structure, there's only one main transformation
        transformation_success = valid_predict
        
        train_entry = {
            'input': grid_to_string_lines(input_grid),
            'output': grid_to_string_lines(expected_output),
            'valid_predict': valid_predict,
            'predict': predict_lines,
            'iou': iou,
            'overlap': overlap,
            'correct': correct,
            'transformation_succeed': [transformation_success]
        }
        train_results.append(train_entry)
    
    return train_results


def process_test_examples(task_data, task_solutions, extracted_json):
    """Process test examples and return results in the new format."""
    test_examples = task_data.get("test", [])
    test_results = []
    
    if not validate_json_structure(extracted_json) or not task_solutions:
        # If JSON is invalid or no solutions, mark all test examples as failed
        for i, example in enumerate(test_examples):
            expected_output = task_solutions[i] if i < len(task_solutions) else []
            test_entry = {
                'input': grid_to_string_lines(example["input"]),
                'output': grid_to_string_lines(expected_output),
                'valid_predict': False,
                'predict': [],
                'iou': 0.0,
                'overlap': 0.0,
                'correct': False,
                'transformation_succeed': [False]
            }
            test_results.append(test_entry)
        return test_results
    
    steps = extracted_json["step_by_step_transformations"]
    if not steps:
        # No steps found, mark all as failed
        for i, example in enumerate(test_examples):
            expected_output = task_solutions[i] if i < len(task_solutions) else []
            test_entry = {
                'input': grid_to_string_lines(example["input"]),
                'output': grid_to_string_lines(expected_output),
                'valid_predict': False,
                'predict': [],
                'iou': 0.0,
                'overlap': 0.0,
                'correct': False,
                'transformation_succeed': [False]
            }
            test_results.append(test_entry)
        return test_results
    
    # Get helper functions from JSON
    helper_functions = extracted_json.get("helper_python_functions", [])
    
    # Use the main python_code from the top level for predictions
    python_code = extracted_json.get("python_code", [])
    
    for i, example in enumerate(test_examples):
        input_grid = example["input"]
        expected_output = task_solutions[i] if i < len(task_solutions) else []
        
        predicted_output, error = execute_transformation_code(python_code, input_grid, helper_functions)
        
        valid_predict = predicted_output is not None
        if valid_predict and expected_output:
            iou = calculate_grid_iou(predicted_output, expected_output)
            overlap = calculate_grid_overlap(predicted_output, expected_output)
            correct = (iou == 100.0 and overlap == 100.0)
            predict_lines = grid_to_string_lines(predicted_output)
        else:
            iou = 0.0
            overlap = 0.0
            correct = False
            predict_lines = grid_to_string_lines(predicted_output) if valid_predict else []
        
        # With the new structure, there's only one main transformation
        transformation_success = valid_predict
        
        test_entry = {
            'input': grid_to_string_lines(input_grid),
            'output': grid_to_string_lines(expected_output),
            'valid_predict': valid_predict,
            'predict': predict_lines,
            'iou': iou,
            'overlap': overlap,
            'correct': correct,
            'transformation_succeed': [transformation_success]
        }
        test_results.append(test_entry)
    
    return test_results


def save_new_format_results(task_id, result_data, output_dir):
    """Save results in the new JSON format."""
    output_file = Path(output_dir) / f"{task_id}.json"
    with open(output_file, 'w') as f:
        json.dump(result_data, f, indent=2)
    return output_file


def collect_execution_errors(task_data, extracted_json):
    """
    Test the JSON code on training examples and collect detailed execution errors.
    
    Args:
        task_data: Task data containing training examples
        extracted_json: The JSON containing the code to test
        
    Returns:
        List of error information for each training example that failed
    """
    training_examples = task_data.get("train", [])
    python_code = extracted_json["python_code"]
    helper_functions = extracted_json.get("helper_python_functions", [])
    execution_errors = []
    
    for i, example in enumerate(training_examples):
        input_grid = example["input"]
        expected_output = example["output"]
        
        try:
            predicted_output, error = execute_transformation_code(python_code, input_grid, helper_functions)
            
            if predicted_output is None:
                execution_errors.append({
                    'input': input_grid,
                    'expected': expected_output,
                    'predicted': None,
                    'error': 'Code execution returned None - likely an execution error or missing return statement'
                })
            elif not is_valid_prediction(predicted_output):
                execution_errors.append({
                    'input': input_grid,
                    'expected': expected_output,
                    'predicted': predicted_output,
                    'error': 'Invalid output format - must be a 2D list with consistent row lengths'
                })
            elif predicted_output != expected_output:
                overlap = calculate_grid_overlap(predicted_output, expected_output)
                execution_errors.append({
                    'input': input_grid,
                    'expected': expected_output,
                    'predicted': predicted_output,
                    'error': f'Incorrect output - only {overlap:.1f}% overlap with expected result'
                })
            # If we get here, the example passed - no error to record
                
        except Exception as e:
            # Try to extract line number and specific error location
            import traceback
            tb_lines = traceback.format_exc().split('\n')
            
            # Look for line numbers in the traceback
            error_line = None
            error_context = str(e)
            
            for line in tb_lines:
                if 'line ' in line and 'transform' in line:
                    # Extract line number from traceback
                    try:
                        line_num = line.split('line ')[1].split(',')[0]
                        error_line = f"Line {line_num}"
                        break
                    except:
                        pass
            
            # Try to find the specific code line that failed
            code_context = ""
            if python_code and error_line:
                try:
                    line_idx = int(line_num) - 1  # Convert to 0-based index
                    if 0 <= line_idx < len(python_code):
                        code_context = f"Failing code line: {python_code[line_idx].strip()}"
                except:
                    pass
            
            error_msg = f'Python execution error: {error_context}'
            if error_line:
                error_msg += f' (at {error_line})'
            if code_context:
                error_msg += f'\n{code_context}'
            
            execution_errors.append({
                'input': input_grid,
                'expected': expected_output,
                'predicted': None,
                'error': error_msg,
                'error_line': error_line,
                'code_context': code_context
            })
    
    return execution_errors


def run_code_repair_attempt(task_id, task_data, model, provider, previous_json, execution_errors, print_input=False, 
                           print_output=False, max_reflections=5):
    """
    Attempt to repair code when JSON is valid but execution fails.
    
    Args:
        task_id: ID of the task
        task_data: Task data
        model: Model to use
        provider: Provider name
        previous_json: The JSON with failing code
        execution_errors: List of execution errors for each training example
        print_input: Whether to print input prompts
        print_output: Whether to print model outputs
        max_reflections: Maximum reflection cycles
        
    Returns:
        Tuple of (new_json, success) where success indicates if code now works
    """
    print(f"\n{BLUE}{'='*60}")
    print("ATTEMPTING CODE REPAIR...")
    print(f"{'='*60}{RESET}")
    
    # Generate code repair prompt
    prompt = build_code_repair_prompt({task_id: task_data}, task_id, previous_json, execution_errors)
    
    print_prompt_in_blue(prompt, print_input)
    
    # Run with selected model using lower temperature for more focused repair
    if model == "llama3.1" or provider == "ollama":
        response = run_with_ollama(prompt, model, temperature_override=0.3)
    elif provider == "google" or provider == "learnlm":
        response = run_with_gemini(prompt, model, temperature_override=0.3)
    else:
        response = run_with_other_model(prompt, model, temperature_override=0.3)
    
    if not response:
        print(f"{RED}No response received for code repair{RESET}")
        return None, False
    
    if print_output:
        print(f"\n{BLUE}Code Repair Response:{RESET}")
        print("-" * 60)
        print(response)
        print("-" * 60)
    
    # Extract JSON from response
    extracted_json = extract_json_from_response(response)
    
    if not extracted_json:
        print(f"{RED}Failed to extract JSON from code repair response{RESET}")
        return None, False
    
    if not validate_json_structure(extracted_json):
        print(f"{RED}Invalid JSON structure in code repair response{RESET}")
        return None, False
    
    print(f"{GREEN}Successfully extracted repaired JSON{RESET}")
    
    # Test the repaired code on training examples
    training_examples = task_data.get("train", [])
    python_code = extracted_json["python_code"]
    helper_functions = extracted_json.get("helper_python_functions", [])
    
    all_successful = True
    for i, example in enumerate(training_examples):
        input_grid = example["input"]
        expected_output = example["output"]
        
        try:
            predicted_output, error = execute_transformation_code(python_code, input_grid, helper_functions)
            
            if predicted_output is None:
                print(f"{RED}âœ— Repaired code still fails on example {i+1}: execution returned None{RESET}")
                all_successful = False
                continue
            
            if not is_valid_prediction(predicted_output):
                print(f"{RED}âœ— Repaired code still fails on example {i+1}: invalid output format{RESET}")
                all_successful = False
                continue
            
            if predicted_output != expected_output:
                overlap = calculate_grid_overlap(predicted_output, expected_output)
                print(f"{RED}âœ— Repaired code still fails on example {i+1}: {overlap:.1f}% overlap{RESET}")
                all_successful = False
                continue
            
            print(f"{GREEN}âœ“ Repaired code works on example {i+1}{RESET}")
            
        except Exception as e:
            print(f"{RED}âœ— Repaired code still fails on example {i+1}: {str(e)}{RESET}")
            all_successful = False
    
    if all_successful:
        print(f"{GREEN}ðŸŽ‰ Code repair successful! All training examples pass.{RESET}")
    else:
        print(f"{RED}Code repair partially successful but some examples still fail.{RESET}")
    
    return extracted_json, all_successful


def run_task_with_reflection(task_id, task_data, solutions, model, provider, previous_result, output_dir=None, 
                            use_sanitized_task=False, use_sanitized_id=True, print_input=False, print_output=False,
                            print_json=True, use_smart_router=True, max_reflections=5, task_retries=2):
    """
    Run a task with reflection prompt based on previous failed attempt.
    Uses the same comprehensive processing pipeline as run_single_task.
    
    Args:
        task_id: ID of the task to run
        task_data: Task data  
        solutions: Solutions data
        model: Model to use
        provider: Provider name
        previous_result: Previous task result containing the failed JSON and train results
        output_dir: Output directory
        use_sanitized_task: Whether to use sanitized task data
        use_sanitized_id: Whether to use sanitized task ID
        print_input: Whether to print input prompts
        print_output: Whether to print model outputs
        print_json: Whether to print JSON
        use_smart_router: Whether to use smart router
        max_reflections: Maximum reflection cycles
        task_retries: Number of times to retry if JSON extraction fails
        
    Returns:
        Result dictionary from run_single_task with reflection
    """
    print(f"\n{'='*80}")
    print(f"REFLECTION ATTEMPT FOR TASK: {task_id}")
    print(f"{'='*80}")
    
    # Apply sanitization if enabled
    if use_sanitized_task:
        from utils import sanitize_task
        working_task_data, working_task_id = sanitize_task(task_data, task_id, use_sanitized_id)
    else:
        working_task_data, working_task_id = task_data, task_id
    
    # Determine the provider if not specified
    if not provider:
        if is_known_model(model):
            provider = MODEL_CONFIGS[model]["provider"]
        else:
            print(f"Unknown model: {model}")
            return None
    
    # Generate reflection prompt instead of regular prompt
    print(f"\nGenerating reflection prompt for task {working_task_id}...")
    previous_json = previous_result.get('transformations_json', {})
    train_results = previous_result.get('trains', [])
    
    prompt = build_arc_reflection_prompt({working_task_id: working_task_data}, working_task_id, previous_json, train_results)
    
    print(f"Task details (ID: {task_id}):")
    print(f"  - Training examples: {len(task_data.get('train', []))}")
    print(f"  - Test examples: {len(task_data.get('test', []))}")
    print(f"  - Previous attempt failed on {len([r for r in train_results if not r.get('correct', False)])} training examples")
    
    # Display the prompt that will be sent to the model
    print_prompt_in_blue(prompt, print_input)
    
    # Count input tokens
    input_tokens = count_tokens_simple(prompt)
    
    # Run with selected model
    if model == "llama3.1" or provider == "ollama":
        response = run_with_ollama(prompt, model)
    elif provider == "google" or provider == "learnlm":
        response = run_with_gemini(prompt, model)
    else:
        response = run_with_other_model(prompt, model)
    
    # Initialize result structure
    result = {
        'total_tokens': 0,
        'input_tokens': input_tokens,
        'output_tokens': 0,
        'estimated_cost': 0.0,
        'transformations_json_generated': False,
        'transformations_json': None,
        'trains': [],
        'tests': [],
        'is_reflection_attempt': True,
        'previous_attempt_trains': train_results
    }
    
    if not response:
        print(f"{RED}No response from model{RESET}")
        return result
    
    # Check if we should enhance the response with smart routing
    initial_json_extracted = extract_json_from_response(response) is not None
    
    # Count output tokens from initial response
    initial_output_tokens = count_tokens_simple(response)
    
    if response and use_smart_router:
        try:
            enhanced_response, additional_tokens = enhance_response_with_smart_routing(
                response, input_tokens, initial_output_tokens, initial_json_extracted, 
                task_id, model, provider, print_input, max_reflections, 
                {working_task_id: working_task_data}
            )
            if enhanced_response and enhanced_response != response:
                print(f"{GREEN}Response enhanced via smart routing{RESET}")
                response = enhanced_response
        except Exception as e:
            print(f"Warning: Smart routing enhancement failed: {e}")
            additional_tokens = 0
    else:
        additional_tokens = 0
    
    # Count output tokens and calculate totals including enhancement tokens
    output_tokens = count_tokens_simple(response)
    total_tokens = input_tokens + output_tokens + additional_tokens
    
    # Calculate estimated cost including enhancement tokens
    try:
        estimated_cost = estimate_cost(model, input_tokens, output_tokens + additional_tokens)
    except KeyError:
        estimated_cost = 0.0
    
    result.update({
        'total_tokens': total_tokens,
        'output_tokens': output_tokens,
        'estimated_cost': estimated_cost
    })
    
    if print_output:
        print(f"\n{BLUE}Model Response (Reflection):{RESET}")
        print("-" * 60)
        print(response)
        print("-" * 60)
    else:
        print(f"Response generated ({count_tokens_simple(response)} tokens)")
    
    # Display token count in blue
    print_token_count_in_blue(response)
    if additional_tokens > 0:
        print(f"{BLUE}Enhancement tokens: {additional_tokens}{RESET}")
    else:
        print(f"{BLUE}No enhancement applied{RESET}")
    
    # Extract and validate JSON
    print(f"\n{'='*60}")
    print("JSON EXTRACTION AND VALIDATION (REFLECTION):")
    print(f"{'='*60}")
    
    extracted_json = extract_json_from_response(response)
    
    if not extracted_json:
        print(f"{RED}Failed to extract valid JSON from reflection response{RESET}")
        print("This would normally trigger retry logic, JSON repair, etc. like in run_single_task")
        # TODO: Could implement the same retry logic as run_single_task here
        return result
    
    print(f"{GREEN}Successfully extracted JSON from reflection!{RESET}")
    result['transformations_json_generated'] = True
    result['transformations_json'] = extracted_json
    
    if print_json:
        print(f"\n{BLUE}Extracted JSON (Reflection):{RESET}")
        print("-" * 60)
        formatted_json = format_json_with_compact_arrays(extracted_json)
        print(formatted_json)
        print("-" * 60)
    elif print_output:
        print(f"{GREEN}JSON extracted successfully (reflection attempt){RESET}")
    else:
        print(f"JSON extracted from reflection ({len(str(extracted_json))} chars)")
    
    # Analyze JSON structure and transformation steps (same as run_single_task)
    print(f"\n{'='*60}")
    print("JSON ANALYSIS (REFLECTION):")
    print(f"{'='*60}")
    
    # 1. Is JSON generated?
    print(f"1. JSON Generated: {GREEN}âœ“ YES{RESET}")
    
    # 2. Is it valid?
    is_valid = validate_json_structure(extracted_json)
    if is_valid:
        print(f"2. Valid Structure: {GREEN}âœ“ YES{RESET}")
    else:
        print(f"2. Valid Structure: {RED}âœ— NO{RESET}")
    
    # 3. How many transformation steps
    if is_valid:
        steps = extracted_json.get("step_by_step_transformations", [])
        print(f"3. Transformation Steps: {len(steps)}")
        
        # Display each step briefly
        for i, step in enumerate(steps, 1):
            step_num = step.get("step_number", i)
            description = step.get("description", ["No description"])
            if isinstance(description, list):
                desc_text = description[0] if description else "No description"
            else:
                desc_text = str(description)
            print(f"   Step {step_num}: {desc_text}")
        
        # 4. Python code present?
        python_code = extracted_json.get("python_code", [])
        if python_code:
            print(f"4. Python Code: {GREEN}âœ“ YES ({len(python_code)} lines){RESET}")
        else:
            print(f"4. Python Code: {RED}âœ— NO{RESET}")
        
        # 5. Helper functions?
        helper_functions = extracted_json.get("helper_python_functions", [])
        if helper_functions:
            print(f"5. Helper Functions: {GREEN}âœ“ YES ({len(helper_functions)} functions){RESET}")
        else:
            print(f"5. Helper Functions: {BLUE}- NONE{RESET}")
        
        # 6. Test code execution and repair if needed
        if python_code:
            print(f"6. Code Execution Test:")
            
            # Test the code on training examples
            training_examples = task_data.get("train", [])
            if training_examples:
                execution_errors = collect_execution_errors(task_data, extracted_json)
                
                if execution_errors:
                    success_count = len(training_examples) - len(execution_errors)
                    total_examples = len(training_examples)
                    print(f"   Execution: {RED}âœ— {success_count}/{total_examples} examples passed{RESET}")
                    
                    # Attempt code repair for reflection attempts too
                    print(f"\n{BLUE}={'='*60}")
                    print("CODE REPAIR NEEDED IN REFLECTION - ATTEMPTING FIX")
                    print(f"={'='*60}{RESET}")
                    
                    print(f"Found {len(execution_errors)} execution errors. Attempting repair...")
                    
                    # Attempt code repair (max 2 attempts for reflection to avoid infinite loops)
                    max_repair_attempts = 2
                    repair_successful = False
                    
                    for repair_attempt in range(max_repair_attempts):
                        print(f"\n{BLUE}Reflection Code Repair Attempt {repair_attempt + 1}/{max_repair_attempts}{RESET}")
                        
                        repaired_json, repair_success = run_code_repair_attempt(
                            task_id, task_data, model, provider, extracted_json, 
                            execution_errors, print_input, print_output, max_reflections
                        )
                        
                        if repair_success and repaired_json:
                            print(f"{GREEN}ðŸŽ‰ Reflection code repair successful! Using repaired JSON.{RESET}")
                            extracted_json = repaired_json
                            result['transformations_json'] = extracted_json
                            repair_successful = True
                            break
                        elif repaired_json:
                            print(f"{BLUE}Partial reflection repair - using improved JSON{RESET}")
                            extracted_json = repaired_json
                            result['transformations_json'] = extracted_json
                            # Check if errors are fixed
                            execution_errors = collect_execution_errors(task_data, extracted_json)
                            if not execution_errors:
                                repair_successful = True
                                break
                        else:
                            print(f"{RED}Reflection code repair attempt {repair_attempt + 1} failed{RESET}")
                    
                    if repair_successful:
                        print(f"{GREEN}Reflection code repair successful after {repair_attempt + 1} attempts{RESET}")
                    else:
                        print(f"{RED}Reflection code repair failed after {max_repair_attempts} attempts{RESET}")
                else:
                    success_count = len(training_examples)
                    total_examples = len(training_examples)
                    print(f"   Execution: {GREEN}âœ“ {success_count}/{total_examples} examples passed{RESET}")
            else:
                print(f"   {RED}No training examples to test{RESET}")
        else:
            print(f"6. Code Execution: {RED}âœ— No Python code to test{RESET}")
    else:
        print(f"3-6. Cannot analyze invalid JSON structure")
    
    # Process training examples with the new JSON
    result['trains'] = process_training_examples(task_data, extracted_json)
    
    # Process test examples  
    result['tests'] = process_test_examples(task_data, solutions.get(task_id, []), extracted_json)
    
    # Save results to output directory if specified
    if output_dir:
        output_file = save_new_format_results(task_id, result, output_dir)
        print(f"{GREEN}Results saved to: {output_file}{RESET}")
    
    return result


def run_task_with_retries(task_id, task_data, solutions, model, provider, output_dir=None, 
                         use_sanitized_task=False, use_sanitized_id=True, print_input=False, print_output=False,
                         print_json=True, use_smart_router=True, max_reflections=5, task_retries=2):
    """
    Run a task with retry logic if JSON extraction fails.
    
    Args:
        task_id: ID of the task to run
        task_data: Task data
        solutions: Solutions data
        model: Model to use
        provider: Provider name
        output_dir: Output directory
        use_sanitized_task: Whether to use sanitized task data
        use_sanitized_id: Whether to use sanitized task ID
        print_input: Whether to print input prompts
        print_output: Whether to print model outputs
        print_json: Whether to print JSON
        use_smart_router: Whether to use smart router
        max_reflections: Maximum reflection cycles
        task_retries: Number of times to retry if JSON extraction fails
        
    Returns:
        Result dictionary from run_single_task
    """
    for attempt in range(task_retries + 1):
        if attempt > 0:
            print(f"\n{BLUE}{'='*80}")
            print(f"RETRY ATTEMPT {attempt}/{task_retries} FOR TASK: {task_id}")
            print(f"Previous attempts failed to generate valid JSON")
            print(f"{'='*80}{RESET}")
        
        # Run the task
        result = run_single_task(
            task_id, task_data, solutions, model, provider, output_dir,
            use_sanitized_task, use_sanitized_id, print_input, print_output,
            print_json, use_smart_router, max_reflections, task_retries
        )
        
        # Check if JSON was successfully generated
        if result.get('transformations_json_generated', False):
            if attempt > 0:
                print(f"\n{GREEN}âœ“ SUCCESS: Task {task_id} succeeded on retry attempt {attempt}{RESET}")
            return result
        
        # If this was the last attempt, return the failed result
        if attempt == task_retries:
            if task_retries > 0:
                print(f"\n{RED}âœ— FAILED: Task {task_id} failed all {task_retries + 1} attempts to generate valid JSON{RESET}")
            return result
        
        # If we have more attempts, prepare for retry
        print(f"\n{RED}âœ— ATTEMPT {attempt + 1} FAILED: Task {task_id} did not generate valid JSON{RESET}")
        if attempt < task_retries:
            print(f"{BLUE}Will retry in next attempt ({attempt + 1}/{task_retries}){RESET}")


def run_single_task(task_id, task_data, solutions, model, provider, output_dir=None, 
                   use_sanitized_task=False, use_sanitized_id=True, print_input=False, print_output=False,
                   print_json=True, use_smart_router=True, max_reflections=5, task_retries=2):
    """Run a single task and return results."""
    print(f"\n{'='*80}")
    print(f"PROCESSING TASK: {task_id}")
    print(f"{'='*80}")
    
    # Apply sanitization if enabled
    if use_sanitized_task:
        print(f"\nSanitizing task {task_id}...")
        sanitized_task_data, sanitized_task_id = sanitize_task(task_data, task_id)
        working_task_data = sanitized_task_data
        
        if use_sanitized_id:
            working_task_id = sanitized_task_id
            print(f"Using sanitized task ID: {working_task_id}")
        else:
            working_task_id = task_id
            print(f"Using original task ID: {working_task_id}")
        
        print("Numbers have been mapped to random characters")
    else:
        print(f"\nUsing original task representation...")
        working_task_data, working_task_id = task_data, task_id
    
    # Determine the provider if not specified
    if not provider:
        from model_configs import MODEL_CONFIGS
        if model in MODEL_CONFIGS:
            provider = MODEL_CONFIGS[model].get("provider", "unknown")
        elif model == "llama3.1":
            provider = "ollama"
        else:
            provider = "unknown"
    
    # Generate prompt
    print(f"\nGenerating prompt for task {working_task_id}...")
    prompt = build_arc_prompt({working_task_id: working_task_data}, working_task_id)
    
    print(f"Task details (ID: {task_id}):")
    print(f"  - Training examples: {len(task_data.get('train', []))}")
    print(f"  - Test examples: {len(task_data.get('test', []))}")
    
    # Display the prompt that will be sent to the model
    print_prompt_in_blue(prompt, print_input)
    
    # Count input tokens
    input_tokens = count_tokens_simple(prompt)
    
    # Run with selected model
    if model == "llama3.1" or provider == "ollama":
        response = run_with_ollama(prompt, model=model)
    elif provider == "google" or provider == "learnlm":
        response = run_with_gemini(prompt, model)
    else:
        response = run_with_other_model(prompt, model)
    
    # Initialize result structure
    result = {
        'total_tokens': 0,
        'input_tokens': input_tokens,
        'output_tokens': 0,
        'estimated_cost': 0.0,
        'transformations_json_generated': False,
        'transformations_json': None,
        'trains': [],
        'tests': []
    }
    
    if not response:
        print("No response received from model.")
        return result
    
    # Check if we should enhance the response with smart routing
    initial_json_extracted = extract_json_from_response(response) is not None
    
    # Count output tokens from initial response
    initial_output_tokens = count_tokens_simple(response)
    
    if response and use_smart_router:
        print(f"\n{BLUE}Checking if response enhancement is needed...{RESET}")
        print(f"Input tokens: {input_tokens}, Output tokens: {initial_output_tokens}, Initial JSON extracted: {'âœ“' if initial_json_extracted else 'âœ—'}")
        
        # Use smart routing to enhance response with any model
        enhanced_response, additional_tokens = enhance_response_with_smart_routing(
            original_response=response,
            input_tokens=input_tokens,
            output_tokens=initial_output_tokens,
            json_extracted=initial_json_extracted,
            task_id=task_id,
            model=model,
            provider=provider,
            print_input=print_input,
            max_reflections=max_reflections,
            task_data=task_data
        )
        
        # Use the enhanced response
        response = enhanced_response
    else:
        # No enhancement used, no additional tokens
        additional_tokens = 0
    
    # Count output tokens and calculate totals including enhancement tokens
    output_tokens = count_tokens_simple(response)
    total_tokens = input_tokens + output_tokens + additional_tokens
    
    # Calculate estimated cost including enhancement tokens
    try:
        estimated_cost = estimate_cost(model, input_tokens, output_tokens + additional_tokens)
    except KeyError:
        estimated_cost = 0.0  # Unknown model or free model
    
    result.update({
        'total_tokens': total_tokens,
        'output_tokens': output_tokens,
        'estimated_cost': estimated_cost
    })
    
    if print_output:
        print(f"\n{'='*60}")
        print("MODEL RESPONSE:")
        print(f"{'='*60}")
        print(response)
        print(f"{'='*60}")
    else:
        print(f"{GREEN}[Model response received - hidden]{RESET}")
    
    # Display token count in blue
    print_token_count_in_blue(response)
    if additional_tokens > 0:
        print(f"Task {task_id}: Input tokens: {input_tokens}, Output tokens: {output_tokens}, Enhancement tokens: +{additional_tokens}, Total: {total_tokens}, Estimated cost: ${estimated_cost:.6f}")
    else:
        print(f"Task {task_id}: Input tokens: {input_tokens}, Output tokens: {output_tokens}, Total: {total_tokens}, Estimated cost: ${estimated_cost:.6f}")
    
    # Extract and validate JSON
    print(f"\n{'='*60}")
    print("JSON EXTRACTION AND VALIDATION:")
    print(f"{'='*60}")
    
    extracted_json = extract_json_from_response(response)
    
    if not extracted_json:
        print(f"{RED}Failed to extract JSON from response{RESET}")
        
        # Show analysis for failed JSON extraction
        print(f"\n{'='*60}")
        print("JSON ANALYSIS:")
        print(f"{'='*60}")
        print(f"1. JSON Generated: {RED}âœ— NO - Failed to extract JSON from response{RESET}")
        print(f"2. JSON Valid: {RED}âœ— NO - No JSON to validate{RESET}")
        print(f"3. Transformation Steps: {RED}Cannot analyze - No JSON{RESET}")
        print(f"4. Steps Analysis: {RED}Cannot analyze - No JSON{RESET}")
        
        # Check if smart router should have been used but wasn't
        if use_smart_router:
            print(f"\n{BLUE}Note: Smart router was used but still couldn't generate valid JSON{RESET}")
        elif not use_smart_router:
            print(f"\n{BLUE}Suggestion: Enable smart router (--use-smart-router) to attempt JSON recovery{RESET}")
        
        result['transformations_json_generated'] = False
        result['transformations_json'] = None
        
        # Still process training and test examples to document the failure
        result['trains'] = process_training_examples(task_data, extracted_json)
        result['tests'] = process_test_examples(task_data, solutions.get(task_id, []), extracted_json)
        
        # Save results to output directory if specified
        if output_dir:
            try:
                save_new_format_results(task_id, result, output_dir)
                print(f"{GREEN}Results saved to: {output_dir}/{task_id}.json{RESET}")
            except Exception as e:
                print(f"{RED}Failed to save results: {e}{RESET}")
        
        return result
    
    print(f"{GREEN}Successfully extracted JSON!{RESET}")
    result['transformations_json_generated'] = True
    result['transformations_json'] = extracted_json
    
    if print_json:
        print(f"\n{BLUE}{'='*70}")
        print("TRANSFORMATION JSON:")
        print(f"{'='*70}{RESET}")
        formatted_json = format_json_with_compact_arrays(extracted_json, indent=2)
        print(formatted_json)
        print(f"{BLUE}{'='*70}{RESET}")
    elif print_output:
        formatted_json = format_json_with_compact_arrays(extracted_json, indent=2)
        print(formatted_json)
    else:
        print(f"{BLUE}[JSON content hidden]{RESET}")
    
    # Analyze JSON structure and transformation steps
    print(f"\n{'='*60}")
    print("JSON ANALYSIS:")
    print(f"{'='*60}")
    
    # 1. Is JSON generated?
    print(f"1. JSON Generated: {GREEN}âœ“ YES{RESET}")
    
    # 2. Is it valid?
    is_valid = validate_json_structure(extracted_json)
    if is_valid:
        print(f"2. JSON Valid: {GREEN}âœ“ YES{RESET}")
    else:
        print(f"2. JSON Valid: {RED}âœ— NO - Missing required structure{RESET}")
    
    # 3. How many transformation steps
    if is_valid:
        steps = extracted_json.get("step_by_step_transformations", [])
        helper_funcs = extracted_json.get("helper_python_functions", [])
        
        num_steps = len(steps)
        num_helpers = len(helper_funcs) if helper_funcs else 0
        
        print(f"3. Transformation Steps: {BLUE}{num_steps} steps{RESET}")
        if num_helpers > 0:
            print(f"   Helper Functions: {BLUE}{num_helpers} functions{RESET}")
        
        # 4. Test main transformation code
        if num_steps > 0:
            print(f"4. Transformation Analysis:")
            
            # Test the main python_code on training examples
            training_examples = task_data.get("train", [])
            if training_examples:
                python_code = extracted_json.get("python_code", [])
                
                # Test main transformation on all training examples
                step_successes = []
                for example in training_examples:
                    predicted_output, error = execute_transformation_code(python_code, example["input"], helper_funcs)
                    success = predicted_output is not None
                    step_successes.append(success)
                
                success_count = sum(step_successes)
                total_examples = len(training_examples)
                success_rate = (success_count / total_examples * 100) if total_examples > 0 else 0
                
                if success_count == total_examples:
                    status_color = GREEN
                    status_symbol = "âœ“"
                elif success_count > 0:
                    status_color = BLUE
                    status_symbol = "~"
                else:
                    status_color = RED
                    status_symbol = "âœ—"
                
                print(f"   Main Transform: {status_color}{status_symbol} {success_count}/{total_examples} ({success_rate:.1f}%){RESET}")
                
                # If code execution failed, attempt code repair
                if success_count < total_examples and is_valid:
                    print(f"\n{BLUE}={'='*60}")
                    print("CODE REPAIR NEEDED - ATTEMPTING AUTOMATIC FIX")
                    print(f"={'='*60}{RESET}")
                    
                    # Collect detailed execution errors
                    execution_errors = collect_execution_errors(task_data, extracted_json)
                    
                    if execution_errors:
                        print(f"Found {len(execution_errors)} execution errors. Attempting repair...")
                        
                        # Attempt code repair (max 3 attempts)
                        max_repair_attempts = 3
                        repair_successful = False
                        
                        for repair_attempt in range(max_repair_attempts):
                            print(f"\n{BLUE}Code Repair Attempt {repair_attempt + 1}/{max_repair_attempts}{RESET}")
                            
                            repaired_json, repair_success = run_code_repair_attempt(
                                task_id, task_data, model, provider, extracted_json, 
                                execution_errors, print_input, print_output, max_reflections
                            )
                            
                            if repair_success and repaired_json:
                                print(f"{GREEN}ðŸŽ‰ Code repair successful! Using repaired JSON.{RESET}")
                                extracted_json = repaired_json
                                result['transformations_json'] = extracted_json
                                repair_successful = True
                                break
                            elif repaired_json:
                                print(f"{BLUE}Partial repair - using improved JSON for next attempt{RESET}")
                                extracted_json = repaired_json
                                result['transformations_json'] = extracted_json
                                # Collect new execution errors for next attempt
                                execution_errors = collect_execution_errors(task_data, extracted_json)
                                if not execution_errors:  # All errors fixed
                                    repair_successful = True
                                    break
                            else:
                                print(f"{RED}Code repair attempt {repair_attempt + 1} failed{RESET}")
                        
                        if repair_successful:
                            print(f"{GREEN}Final result: Code repair successful after {repair_attempt + 1} attempts{RESET}")
                        else:
                            print(f"{RED}Final result: Code repair failed after {max_repair_attempts} attempts{RESET}")
                    
            else:
                print(f"   {RED}No training examples to test transformation{RESET}")
        else:
            print(f"4. Transformation Analysis: {RED}No transformation steps found{RESET}")
    else:
        print(f"3. Transformation Steps: {RED}Cannot analyze - Invalid JSON structure{RESET}")
        print(f"4. Steps Analysis: {RED}Cannot analyze - Invalid JSON structure{RESET}")
    
    # Process training examples
    result['trains'] = process_training_examples(task_data, extracted_json)
    
    # Process test examples  
    result['tests'] = process_test_examples(task_data, solutions.get(task_id, []), extracted_json)
    
    # Save results to output directory if specified
    if output_dir:
        try:
            save_new_format_results(task_id, result, output_dir)
            print(f"{GREEN}Results saved to: {output_dir}/{task_id}.json{RESET}")
        except Exception as e:
            print(f"{RED}Failed to save results: {e}{RESET}")
    
    return result


def run_single_task_wrapper(args):
    """Wrapper function for parallel execution with output capturing."""
    task_id, task_data, solutions, model, provider, output_dir, use_sanitized_task, use_sanitized_id, print_input, print_output, print_json, use_smart_router, max_reflections, task_retries = args
    
    # Capture all output for this task
    with OutputCapture() as captured_output:
        result = run_task_with_retries(task_id, task_data, solutions, model, provider, output_dir, 
                                      use_sanitized_task, use_sanitized_id, print_input, print_output, 
                                      print_json, use_smart_router, max_reflections, task_retries)
    
    # Return both the result and the captured output
    output_text = captured_output.getvalue()
    return result, output_text, task_id


def save_task_results(task_id, task_data, solutions, predicted_outputs, output_dir):
    """Save test inputs, expected outputs, and predicted outputs for a task."""
    task_output_dir = Path(output_dir) / task_id
    task_output_dir.mkdir(parents=True, exist_ok=True)
    
    test_examples = task_data.get("test", [])
    task_solutions = solutions.get(task_id, [])
    
    # Save each test example
    for i, (test_example, expected_output) in enumerate(zip(test_examples, task_solutions)):
        test_input = test_example["input"]
        predicted_output = predicted_outputs[i] if i < len(predicted_outputs) else None
        
        # Create result data
        result_data = {
            "task_id": task_id,
            "test_index": i,
            "test_input": test_input,
            "test_output": expected_output,
            "produced_output": predicted_output,
            "input_size": [len(test_input), len(test_input[0]) if test_input else 0],
            "expected_size": [len(expected_output), len(expected_output[0]) if expected_output else 0],
            "produced_size": [len(predicted_output), len(predicted_output[0]) if predicted_output else 0] if predicted_output else None
        }
        
        # Save to JSON file
        output_file = task_output_dir / f"test_{i}.json"
        with open(output_file, 'w') as f:
            json.dump(result_data, f, indent=2)
    
    return task_output_dir


def calculate_matching_criteria(predicted, expected):
    """Calculate various matching criteria between predicted and expected outputs."""
    if not predicted or not expected:
        return {
            'overlap_percent': 0.0,
            'exact_match': False,
            'size_match': False,
            'shape_match': False
        }
    
    # Size matching
    pred_height, pred_width = len(predicted), len(predicted[0]) if predicted else 0
    exp_height, exp_width = len(expected), len(expected[0]) if expected else 0
    size_match = (pred_height == exp_height and pred_width == exp_width)
    shape_match = size_match  # Same as size match for 2D grids
    
    if not size_match:
        return {
            'overlap_percent': 0.0,
            'exact_match': False,
            'size_match': False,
            'shape_match': False
        }
    
    # Calculate overlap
    total_cells = pred_height * pred_width
    matching_cells = 0
    
    for i in range(pred_height):
        for j in range(pred_width):
            if predicted[i][j] == expected[i][j]:
                matching_cells += 1
    
    overlap_percent = (matching_cells / total_cells) * 100.0 if total_cells > 0 else 0.0
    exact_match = overlap_percent >= 100.0
    
    return {
        'overlap_percent': overlap_percent,
        'exact_match': exact_match,
        'size_match': size_match,
        'shape_match': shape_match
    }


def test_summary(output_dir):
    """Generate summary from saved test results."""
    output_path = Path(output_dir)
    if not output_path.exists():
        print(f"{RED}Output directory not found: {output_dir}{RESET}")
        return
    
    print(f"\n{'='*100}")
    print(f"TEST SUMMARY FROM: {output_dir}")
    print(f"{'='*100}")
    
    # Find all task directories
    task_dirs = [d for d in output_path.iterdir() if d.is_dir()]
    if not task_dirs:
        print(f"{RED}No task directories found in {output_dir}{RESET}")
        return
    
    all_results = []
    
    for task_dir in task_dirs:
        task_id = task_dir.name
        test_files = sorted(task_dir.glob("test_*.json"))
        
        task_results = {
            'task_id': task_id,
            'test_results': [],
            'overall_exact_match': True,
            'overall_size_match': True,
            'overall_overlap': 0.0
        }
        
        total_overlap = 0.0
        
        for test_file in test_files:
            with open(test_file, 'r') as f:
                test_data = json.load(f)
            
            predicted = test_data.get('produced_output')
            expected = test_data.get('test_output')
            
            criteria = calculate_matching_criteria(predicted, expected)
            test_data['matching_criteria'] = criteria
            task_results['test_results'].append(test_data)
            
            if not criteria['exact_match']:
                task_results['overall_exact_match'] = False
            if not criteria['size_match']:
                task_results['overall_size_match'] = False
            
            total_overlap += criteria['overlap_percent']
        
        if test_files:
            task_results['overall_overlap'] = total_overlap / len(test_files)
        
        all_results.append(task_results)
    
    # Calculate overall statistics
    total_tasks = len(all_results)
    exact_match_tasks = sum(1 for r in all_results if r['overall_exact_match'])
    size_match_tasks = sum(1 for r in all_results if r['overall_size_match'])
    completed_tasks = sum(1 for r in all_results if any(tr.get('produced_output') for tr in r['test_results']))
    
    total_overlap_sum = sum(r['overall_overlap'] for r in all_results)
    overall_avg_overlap = total_overlap_sum / total_tasks if total_tasks > 0 else 0.0
    
    # Display summary
    print(f"\n{BLUE}SUMMARY STATISTICS:{RESET}")
    print(f"  Total tasks analyzed: {total_tasks}")
    print(f"  Tasks with completed predictions: {completed_tasks}")
    print(f"  Completion rate: {(completed_tasks / total_tasks * 100):.1f}%")
    print(f"  Tasks with exact matches (100%): {exact_match_tasks}")
    print(f"  Exact match rate: {(exact_match_tasks / total_tasks * 100):.1f}%")
    print(f"  Tasks with matching sizes: {size_match_tasks}")
    print(f"  Size match rate: {(size_match_tasks / total_tasks * 100):.1f}%")
    print(f"  Overall average overlap: {overall_avg_overlap:.1f}%")
    
    # Detailed task results
    print(f"\n{BLUE}DETAILED TASK RESULTS:{RESET}")
    for result in all_results:
        status_exact = "âœ“" if result['overall_exact_match'] else "âœ—"
        status_size = "âœ“" if result['overall_size_match'] else "âœ—"
        overlap = result['overall_overlap']
        
        print(f"  {status_exact} {result['task_id']} - Exact:{status_exact} Size:{status_size} Overlap:{overlap:.1f}%")
    
    return all_results


def load_arc_tasks(filepath):
    """Load ARC tasks from JSON file."""
    with open(filepath, 'r') as f:
        return json.load(f)


def load_arc_solutions(filepath):
    """Load ARC solutions from JSON file."""
    with open(filepath, 'r') as f:
        return json.load(f)


def get_task_by_index(tasks, index=None):
    """Get a task by index, task ID string, or randomly select one if index is None."""
    task_ids = list(tasks.keys())
    
    if index is None:
        # Random selection
        selected_id = random.choice(task_ids)
        print(f"Randomly selected task: {selected_id}")
    elif isinstance(index, str) and not (index.isdigit() and len(index) <= 4):
        # Task ID string (not a simple numeric index)
        if index in tasks:
            selected_id = index
            print(f"Selected task by ID: {selected_id}")
        else:
            raise ValueError(f"Task ID '{index}' not found. Available tasks: {len(task_ids)} total")
    else:
        # Numeric index (either int or string that represents a reasonable index)
        numeric_index = int(index)
        if numeric_index < 0 or numeric_index >= len(task_ids):
            raise ValueError(f"Index {numeric_index} out of range. Available tasks: 0-{len(task_ids)-1}")
        selected_id = task_ids[numeric_index]
        print(f"Selected task by index {numeric_index}: {selected_id}")
    
    return selected_id, tasks[selected_id]


def get_multiple_tasks(tasks, num_tasks, specific_index=None):
    """Get multiple tasks for batch processing."""
    task_ids = list(tasks.keys())
    selected_tasks = []
    
    if specific_index is not None:
        # Single specific task (either index or task ID)
        if isinstance(specific_index, str) and not (specific_index.isdigit() and len(specific_index) <= 4):
            # Task ID string (not a simple numeric index)
            if specific_index in tasks:
                selected_id = specific_index
                print(f"Selected specific task by ID: {selected_id}")
            else:
                raise ValueError(f"Task ID '{specific_index}' not found. Available tasks: {len(task_ids)} total")
        else:
            # Numeric index
            numeric_index = int(specific_index)
            if numeric_index < 0 or numeric_index >= len(task_ids):
                raise ValueError(f"Index {numeric_index} out of range. Available tasks: 0-{len(task_ids)-1}")
            selected_id = task_ids[numeric_index]
            print(f"Selected specific task by index {numeric_index}: {selected_id}")
        
        selected_tasks.append((selected_id, tasks[selected_id]))
    else:
        # Random selection or all tasks
        if num_tasks == -1:
            # Run all tasks
            num_tasks = len(task_ids)
            selected_ids = task_ids
            print(f"Running ALL {num_tasks} tasks")
        else:
            # Random selection
            if num_tasks > len(task_ids):
                num_tasks = len(task_ids)
                print(f"Limiting to {num_tasks} tasks (max available)")
            
            selected_ids = random.sample(task_ids, num_tasks)
        for task_id in selected_ids:
            selected_tasks.append((task_id, tasks[task_id]))
        
        print(f"Randomly selected {len(selected_tasks)} tasks: {[tid for tid, _ in selected_tasks]}")
    
    return selected_tasks


def run_with_ollama(prompt, model="llama3.1", temperature_override=None):
    """Run the prompt with Ollama using API if available, otherwise fallback to CLI."""
    
    # Use override temperature if provided, otherwise use global TEMPERATURE
    effective_temperature = temperature_override if temperature_override is not None else TEMPERATURE
    
    print(f"\n{'='*60}")
    print(f"Running with Ollama model: {model}")
    print(f"Temperature: {effective_temperature}")
    print(f"Max tokens: {'unlimited' if NUM_PREDICT == -1 else NUM_PREDICT}")
    print(f"{'='*60}")
    
    try:
        # Use Ollama API
        api_url = "http://localhost:11434/api/generate"
        
        # Prepare options - only include num_predict if it's not -1
        options = {"temperature": effective_temperature}
        if NUM_PREDICT != -1:
            options["num_predict"] = NUM_PREDICT
        
        data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": options
        }
        
        response = requests.post(api_url, json=data, timeout=300)
        
        if response.status_code == 200:
            result = response.json()
            return result.get('response', '')
        else:
            print(f"Error: HTTP {response.status_code}")
            print(f"Response: {response.text}")
            return None
            
    except requests.exceptions.ConnectionError:
        print("Error: Cannot connect to Ollama API. Make sure Ollama is running.")
        print("You can start it with: ollama serve")
        print("Falling back to CLI...")
    except Exception as e:
        print(f"Error with Ollama API: {e}")
        print("Falling back to CLI...")
    
    # Fallback to simple ollama run without parameters
    try:
        print("Using simple ollama run command (parameters not supported)...")
        cmd = ["ollama", "run", model]
        
        result = subprocess.run(
            cmd,
            input=prompt,
            text=True,
            capture_output=True,
            check=True
        )
        
        return result.stdout
        
    except subprocess.CalledProcessError as e:
        print(f"Error running Ollama: {e}")
        print(f"Stderr: {e.stderr}")
        return None
    except FileNotFoundError:
        print("Error: Ollama not found. Please install Ollama first.")
        return None


def run_with_gemini(prompt, model, temperature_override=None):
    """Run the prompt with Google Gemini API."""
    
    # Use override temperature if provided, otherwise use global TEMPERATURE
    effective_temperature = temperature_override if temperature_override is not None else TEMPERATURE
    
    print(f"\n{'='*60}")
    print(f"Running with Gemini model: {model}")
    print(f"Temperature: {effective_temperature}")
    print(f"Max tokens: {'unlimited' if NUM_PREDICT == -1 else NUM_PREDICT}")
    print(f"{'='*60}")
    

    
    # Get API key from environment
    api_key = os.getenv('GEMINI_API_KEY')
    if not api_key:
        print("Error: GEMINI_API_KEY environment variable not set.")
        print("Please set your Gemini API key:")
        print("export GEMINI_API_KEY='your-api-key-here'")
        return None
    
    try:
        # Import genai locally to avoid ALTS warnings at module level
        import google.generativeai as genai
        
        # Configure the API
        genai.configure(api_key=api_key)
        
        # Create the model with generation config
        generation_config = {"temperature": effective_temperature}
        if NUM_PREDICT != -1:
            generation_config["max_output_tokens"] = NUM_PREDICT
        
        model_instance = genai.GenerativeModel(
            model_name=model,
            generation_config=generation_config
        )
        
        # Generate response
        response = model_instance.generate_content(prompt)
        
        if response.text:
            return response.text
        else:
            print("Error: No response text received from Gemini")
            if hasattr(response, 'prompt_feedback'):
                print(f"Prompt feedback: {response.prompt_feedback}")
            return None
            
    except Exception as e:
        print(f"Error calling Gemini API: {e}")
        return None


def run_with_other_model(prompt, model, temperature_override=None):
    """Placeholder for other model integrations."""
    # Use override temperature if provided, otherwise use global TEMPERATURE
    effective_temperature = temperature_override if temperature_override is not None else TEMPERATURE
    
    print(f"\n{'='*60}")
    print(f"Model '{model}' integration not implemented yet.")
    print(f"This would use the model configuration:")
    print(f"Provider: {MODEL_CONFIGS[model].get('provider', 'unknown')}")
    print(f"Description: {MODEL_CONFIGS[model].get('description', 'No description')}")
    print(f"Temperature: {effective_temperature}")
    print(f"{'='*60}")
    
    # For now, just print the prompt
    print("\nGenerated prompt:")
    print(prompt)
    
    return "Model integration not implemented yet."

def test_json_execution_comprehensive(extracted_json, task_data):
    """Test if the JSON can execute successfully on ALL training examples with detailed error reporting."""
    if not validate_json_structure(extracted_json):
        return False, "Invalid JSON structure", None
    
    steps = extracted_json.get("step_by_step_transformations", [])
    if not steps:
        return False, "No transformation steps found", None
    
    # Get helper functions and main python_code
    helper_functions = extracted_json.get("helper_python_functions", [])
    python_code = extracted_json.get("python_code", [])
    
    # Test on ALL training examples
    training_examples = task_data.get("train", [])
    if not training_examples:
        return False, "No training examples found", None
    
    print(f"\n{BLUE}ðŸ§ª COMPREHENSIVE CODE TESTING{RESET}")
    print(f"Testing code on {len(training_examples)} training examples...")
    
    execution_results = []
    
    for i, example in enumerate(training_examples):
        input_grid = example["input"]
        expected_output = example["output"]
        
        print(f"\n  ðŸ“ Training Example {i+1}:")
        print(f"     Input size: {len(input_grid)}x{len(input_grid[0]) if input_grid else 0}")
        print(f"     Expected output size: {len(expected_output)}x{len(expected_output[0]) if expected_output else 0}")
        print(f"     Input grid: {input_grid}")
        
        predicted_output, error = execute_transformation_code(python_code, input_grid, helper_functions)
        
        if predicted_output is None:
            print(f"     {RED}âŒ EXECUTION FAILED{RESET}")
            print(f"     {RED}Error: {error}{RESET}")
            execution_results.append({
                'example_idx': i,
                'input': input_grid,
                'expected_output': expected_output,
                'success': False,
                'error': error,
                'predicted_output': None
            })
        else:
            overlap = calculate_grid_overlap(predicted_output, expected_output)
            print(f"     {GREEN}âœ… EXECUTION SUCCESS{RESET}")
            print(f"     Predicted size: {len(predicted_output)}x{len(predicted_output[0]) if predicted_output else 0}")
            print(f"     Overlap: {overlap:.1f}%")
            print(f"     Predicted grid: {predicted_output}")
            
            execution_results.append({
                'example_idx': i,
                'input': input_grid,
                'expected_output': expected_output,
                'success': True,
                'error': None,
                'predicted_output': predicted_output,
                'overlap': overlap
            })
    
    # Check if any failed
    failed_examples = [r for r in execution_results if not r['success']]
    if failed_examples:
        print(f"\n{RED}ðŸ’¥ CODE EXECUTION FAILURES: {len(failed_examples)}/{len(training_examples)} examples failed{RESET}")
        return False, failed_examples[0]['error'], failed_examples[0]  # Return first failure for repair
    else:
        successful_count = len([r for r in execution_results if r['success']])
        avg_overlap = sum(r.get('overlap', 0) for r in execution_results if r['success']) / successful_count if successful_count > 0 else 0
        print(f"\n{GREEN}ðŸŽ¯ ALL TRAINING EXAMPLES PASSED!{RESET}")
        print(f"   Success rate: {successful_count}/{len(training_examples)} ({successful_count/len(training_examples)*100:.1f}%)")
        print(f"   Average overlap: {avg_overlap:.1f}%")
        return True, None, None


def is_reasoning_complete(response_text):
    """Check if the reasoning appears to be complete based on text patterns."""
    if not response_text:
        return False
    
    # Convert to lowercase for pattern matching
    text = response_text.lower()
    
    # Check for completion indicators
    completion_patterns = [
        'in conclusion',
        'final rule',
        'final transformation',
        'therefore, the rule',
        'so the pattern',
        'the transformation rule is',
        'applying this rule',
        'final json',
        'json response',
        '```json',
        'step_by_step_transformations'
    ]
    
    has_completion_indicator = any(pattern in text for pattern in completion_patterns)
    
    # Check if it ends abruptly (incomplete)
    incomplete_endings = [
        'the pattern seems to',
        'we can see that',
        'this suggests',
        'looking at',
        'analyzing',
        'examining',
        'considering',
        'based on this'
    ]
    
    # Get the last few words to check for incomplete endings
    words = text.split()
    if len(words) > 10:
        last_portion = ' '.join(words[-10:])
        ends_incomplete = any(ending in last_portion for ending in incomplete_endings)
    else:
        ends_incomplete = False
    
    # Reasoning is complete if it has completion indicators and doesn't end abruptly
    return has_completion_indicator and not ends_incomplete


def call_model(prompt, model, provider=None, temperature=None):
    """
    Generic function to call any model with the appropriate provider.
    
    Args:
        prompt (str): The prompt to send to the model
        model (str): The model name
        provider (str, optional): The provider name, if not auto-detected
        temperature (float, optional): Temperature override for this call
        
    Returns:
        str: The model response, or None if failed
    """
    # Show when reflection temperature is being used
    if temperature is not None and temperature != TEMPERATURE:
        print(f"ðŸ”§ Using reflection temperature: {temperature} (instead of main temperature: {TEMPERATURE})")
    
    # Auto-detect provider if not specified
    if provider is None:
        from model_configs import MODEL_CONFIGS
        if model in MODEL_CONFIGS:
            provider = MODEL_CONFIGS[model].get("provider", "unknown")
        elif model == "llama3.1":
            provider = "ollama"
        else:
            provider = "unknown"
    
    # Route to the appropriate model function
    if provider == "ollama" or model == "llama3.1":
        return run_with_ollama(prompt, model, temperature)
    elif provider == "google" or provider == "learnlm":
        return run_with_gemini(prompt, model, temperature)
    else:
        return run_with_other_model(prompt, model, temperature)


def enhance_response_with_smart_routing(original_response, input_tokens, output_tokens, json_extracted, task_id, model, provider=None, print_input=False, max_reflections=5, task_data=None):
    """
    Enhance the response by continuing reasoning or fixing JSON/code issues using any model with recursive reflection.
    
    Args:
        original_response (str): The original model response
        input_tokens (int): Number of input tokens in the original prompt
        output_tokens (int): Number of output tokens in the original response
        json_extracted (bool): Whether valid JSON was extracted from the response
        task_id (str): Task identifier for logging
        model (str): The model to use for enhancement
        provider (str, optional): The provider name, if not auto-detected
        print_input (bool): Whether to print prompts
        max_reflections (int): Maximum number of reflection cycles for complete enhancement
        task_data (dict, optional): Task data for testing code execution
        
    Returns:
        tuple: (Enhanced response with fixed JSON and code, additional tokens consumed)
    """
    print(f"\n{BLUE}{'='*60}")
    print("SMART RESPONSE ENHANCEMENT WITH RECURSIVE REFLECTION")
    print(f"Using model: {model} (provider: {provider or 'auto-detect'})")
    print(f"Max reflection cycles: {max_reflections}")
    print(f"{'='*60}{RESET}")
    
    print(f"Initial conditions:")
    print(f"  - Input tokens: {input_tokens}")
    print(f"  - Output tokens: {output_tokens}")
    print(f"  - JSON initially extracted: {'âœ“' if json_extracted else 'âœ—'}")
    print(f"  - Reasoning complete: {'âœ“' if is_reasoning_complete(original_response) else 'âœ—'}")
    
    current_response = original_response
    reflection_cycle = 0
    enhancement_actions = []
    additional_tokens = 0  # Track additional tokens consumed during enhancement
    
    # Main reflection loop - continue until we have working code or hit max reflections
    while reflection_cycle < max_reflections:
        reflection_cycle += 1
        print(f"\n{BLUE}ðŸ”„ REFLECTION CYCLE {reflection_cycle}/{max_reflections}{RESET}")
        
        cycle_enhanced = False
        
        # STEP 1: Handle incomplete reasoning (reasoning continuation)
        # Stop reasoning continuation 2 cycles before max reflections to force JSON
        max_continuation_cycles = max_reflections - 2
        needs_continuation = (output_tokens > 500 and not is_reasoning_complete(current_response) and reflection_cycle <= max_continuation_cycles)
        if needs_continuation:
            print(f"\n{BLUE}ï¿½ REASONING CONTINUATION NEEDED{RESET}")
            print(f"Reason: High output token count ({output_tokens}) + incomplete reasoning (cycle {reflection_cycle}/{max_continuation_cycles})")
            enhancement_actions.append(f"Reasoning Continuation (Cycle {reflection_cycle})")
            
            continuation_prompt = create_continuation_prompt(current_response)
            print_prompt_in_blue(continuation_prompt, print_input)
            
            continuation_response = call_model(continuation_prompt, model, provider, REFLECTION_TEMPERATURE)
            
            if continuation_response:
                # Count tokens for this additional call
                continuation_input_tokens = count_tokens_simple(continuation_prompt)
                continuation_output_tokens = count_tokens_simple(continuation_response)
                additional_tokens += continuation_input_tokens + continuation_output_tokens
                
                current_response += "\n\n" + continuation_response
                cycle_enhanced = True
                print(f"  {GREEN}âœ… Reasoning continued successfully (+{continuation_input_tokens + continuation_output_tokens} tokens){RESET}")
            else:
                print(f"  {RED}âŒ Failed to get continuation response{RESET}")
        
        # STEP 2: Handle JSON extraction and validation issues
        extracted_json = extract_json_from_response(current_response)
        
        if is_reasoning_complete(current_response) and not extracted_json:
            # SCENARIO 1: Reasoning complete but no JSON found
            print(f"\n{BLUE}ðŸ”§ JSON GENERATION NEEDED{RESET}")
            print("Reason: Reasoning complete but no JSON found")
            enhancement_actions.append(f"JSON Generation (Cycle {reflection_cycle})")
            
            json_regen_prompt = create_json_regeneration_prompt(current_response)
            print_prompt_in_blue(json_regen_prompt, print_input)
            
            json_response = call_model(json_regen_prompt, model, provider, REFLECTION_TEMPERATURE)
            
            if json_response:
                # Count tokens for this additional call
                json_input_tokens = count_tokens_simple(json_regen_prompt)
                json_output_tokens = count_tokens_simple(json_response)
                additional_tokens += json_input_tokens + json_output_tokens
                
                current_response += "\n\n" + json_response
                cycle_enhanced = True
                extracted_json = extract_json_from_response(current_response)
                print(f"  {GREEN}âœ… JSON generation attempt completed (+{json_input_tokens + json_output_tokens} tokens){RESET}")
                
                # Print the generated JSON for debugging
                if extracted_json:
                    print(f"\n{BLUE}{'='*60}")
                    print("GENERATED JSON (COMPACT):")
                    print(f"{'='*60}{RESET}")
                    formatted_json = format_json_with_compact_arrays(extracted_json, indent=2)
                    print(formatted_json)
                    print(f"{BLUE}{'='*60}{RESET}")
            else:
                print(f"  {RED}âŒ Failed to get JSON generation response{RESET}")
        
        # SCENARIO 2: JSON found but invalid/malformed
        if extracted_json is None and '```json' in current_response:
            # Extract the raw JSON text to see what's wrong
            import re
            json_pattern = r'```json\s*\n(.*?)\n```'
            matches = re.findall(json_pattern, current_response, re.DOTALL)
            if matches:
                raw_json = matches[-1].strip()  # Get the last JSON block
                try:
                    json.loads(raw_json)  # This should fail
                except json.JSONDecodeError as e:
                    print(f"\n{BLUE}ðŸ”§ JSON REPAIR NEEDED{RESET}")
                    print(f"Reason: JSON syntax error - {str(e)}")
                    enhancement_actions.append(f"JSON Repair (Cycle {reflection_cycle})")
                    
                    json_repair_prompt = create_json_repair_prompt(raw_json, str(e))
                    print_prompt_in_blue(json_repair_prompt, print_input)
                    
                    repair_response = call_model(json_repair_prompt, model, provider, REFLECTION_TEMPERATURE)
                    
                    if repair_response:
                        # Count tokens for this additional call
                        repair_input_tokens = count_tokens_simple(json_repair_prompt)
                        repair_output_tokens = count_tokens_simple(repair_response)
                        additional_tokens += repair_input_tokens + repair_output_tokens
                        
                        current_response += "\n\n" + repair_response
                        cycle_enhanced = True
                        extracted_json = extract_json_from_response(current_response)
                        print(f"  {GREEN}âœ… JSON repair attempt completed (+{repair_input_tokens + repair_output_tokens} tokens){RESET}")
                        
                        # Print the repaired JSON for debugging
                        if extracted_json:
                            print(f"\n{BLUE}{'='*60}")
                            print("REPAIRED JSON (COMPACT):")
                            print(f"{'='*60}{RESET}")
                            formatted_json = format_json_with_compact_arrays(extracted_json, indent=2)
                            print(formatted_json)
                            print(f"{BLUE}{'='*60}{RESET}")
                    else:
                        print(f"  {RED}âŒ Failed to get JSON repair response{RESET}")
        
        # SCENARIO 3: Valid JSON but code execution fails
        if extracted_json and task_data:
            print(f"\n{BLUE}ðŸ§ª TESTING JSON CODE EXECUTION{RESET}")
            can_execute, execution_error, failed_example = test_json_execution_comprehensive(extracted_json, task_data)
            
            if not can_execute:
                print(f"\n{BLUE}ðŸ”§ CODE REPAIR NEEDED{RESET}")
                print(f"Reason: Code execution failed")
                enhancement_actions.append(f"Code Repair (Cycle {reflection_cycle})")
                
                # Get all failure details if available
                all_failures = []
                if isinstance(execution_error, list):  # Multiple failures
                    all_failures = execution_error
                    primary_error = execution_error[0]['error'] if execution_error else "Multiple execution failures"
                    failed_example = execution_error[0] if execution_error else failed_example
                else:
                    primary_error = execution_error
                    if failed_example:
                        all_failures = [failed_example]
                
                code_repair_prompt = create_enhanced_code_repair_prompt(extracted_json, primary_error, failed_example, all_failures)
                print_prompt_in_blue(code_repair_prompt, print_input)
                
                repair_response = call_model(code_repair_prompt, model, provider, REFLECTION_TEMPERATURE)
                
                if repair_response:
                    # Count tokens for this additional call
                    code_repair_input_tokens = count_tokens_simple(code_repair_prompt)
                    code_repair_output_tokens = count_tokens_simple(repair_response)
                    additional_tokens += code_repair_input_tokens + code_repair_output_tokens
                    
                    current_response += "\n\n" + repair_response
                    cycle_enhanced = True
                    extracted_json = extract_json_from_response(current_response)
                    print(f"  {GREEN}âœ… Code repair attempt completed (+{code_repair_input_tokens + code_repair_output_tokens} tokens){RESET}")
                    
                    # Print the repaired JSON for debugging
                    if extracted_json:
                        print(f"\n{BLUE}{'='*60}")
                        print("CODE REPAIRED JSON (COMPACT):")
                        print(f"{'='*60}{RESET}")
                        formatted_json = format_json_with_compact_arrays(extracted_json, indent=2)
                        print(formatted_json)
                        print(f"{BLUE}{'='*60}{RESET}")
                    
                    # Test the repaired code
                    if extracted_json:
                        can_execute_after, _, _ = test_json_execution_comprehensive(extracted_json, task_data)
                        if can_execute_after:
                            print(f"  {GREEN}ðŸŽ¯ CODE REPAIR SUCCESSFUL! All examples now pass{RESET}")
                            break  # Success! Exit reflection loop
                        else:
                            print(f"  {RED}ðŸ”„ Code still has issues, will continue reflection...{RESET}")
                            # Add additional guidance for the next iteration
                            if reflection_cycle < max_reflections:
                                print(f"  {BLUE}ðŸ’¡ Tip: Ensure each step has a complete transform(grid) function{RESET}")
                else:
                    print(f"  {RED}âŒ Failed to get code repair response{RESET}")
            else:
                print(f"  {GREEN}ðŸŽ¯ CODE EXECUTION SUCCESSFUL! All examples pass{RESET}")
                break  # Success! Exit reflection loop
        
        # Check if we made any progress this cycle
        if not cycle_enhanced:
            print(f"\n{BLUE}ðŸ No enhancement needed or possible in this cycle{RESET}")
            break
    
    # Final status report
    final_json_extracted = extract_json_from_response(current_response) is not None
    final_code_working = False
    
    if final_json_extracted and task_data:
        final_extracted_json = extract_json_from_response(current_response)
        can_execute_final, _, _ = test_json_execution_comprehensive(final_extracted_json, task_data)
        final_code_working = can_execute_final
    
    print(f"\n{BLUE}{'='*60}")
    print("ENHANCEMENT SUMMARY")
    print(f"{'='*60}{RESET}")
    print(f"Reflection cycles completed: {reflection_cycle}/{max_reflections}")
    print(f"Enhancement actions taken: {', '.join(enhancement_actions) if enhancement_actions else 'None'}")
    print(f"Final JSON extracted: {'âœ“' if final_json_extracted else 'âœ—'}")
    print(f"Final code working: {'âœ“' if final_code_working else 'âœ—'}")
    
    if final_code_working:
        print(f"{GREEN}ðŸŽ‰ ENHANCEMENT COMPLETE: Fully working solution achieved!{RESET}")
    elif final_json_extracted:
        print(f"{BLUE}ðŸ“ PARTIAL SUCCESS: JSON extracted but code may have issues{RESET}")
    else:
        print(f"{RED}âŒ ENHANCEMENT INCOMPLETE: Could not achieve working solution{RESET}")
    
    print(f"Total additional tokens consumed during enhancement: {additional_tokens}")
    
    return current_response, additional_tokens

def check_existing_output_folder(existing_output_path):
    """
    Check if the existing output folder is valid for continuing a run.
    
    Args:
        existing_output_path (str): Path to the existing output folder
        
    Returns:
        tuple: (is_valid, completed_tasks, params_data)
               is_valid: bool indicating if the folder is valid
               completed_tasks: set of task IDs that have been completed
               params_data: dict of parameters from the previous run (or None)
    """
    if not existing_output_path:
        return False, set(), None
    
    output_path = Path(existing_output_path)
    
    # Check if the folder exists
    if not output_path.exists():
        print(f"{RED}Existing output folder does not exist: {existing_output_path}{RESET}")
        return False, set(), None
    
    if not output_path.is_dir():
        print(f"{RED}Existing output path is not a directory: {existing_output_path}{RESET}")
        return False, set(), None
    
    # Check for params.json to verify it's a valid output folder
    params_file = output_path / 'params.json'
    params_data = None
    if params_file.exists():
        try:
            with open(params_file, 'r') as f:
                params_data = json.load(f)
            print(f"{GREEN}Found params.json from previous run{RESET}")
        except Exception as e:
            print(f"{RED}Warning: Could not read params.json: {e}{RESET}")
    else:
        print(f"{BLUE}No params.json found - assuming this is a valid output folder{RESET}")
    
    # Scan for completed task JSON files
    completed_tasks = set()
    json_files = list(output_path.glob("*.json"))
    
    # Filter out non-task files
    task_files = [f for f in json_files if f.name not in ['params.json', 'summary.json', 'task_ids.json']]
    
    for task_file in task_files:
        # Task files should be named like "task_id.json"
        task_id = task_file.stem
        
        # Validate that it's a proper task result file
        try:
            with open(task_file, 'r') as f:
                task_data = json.load(f)
            
            # Check if it has the expected structure
            if isinstance(task_data, dict) and any(key in task_data for key in ['trains', 'tests', 'transformations_json']):
                completed_tasks.add(task_id)
                print(f"{GREEN}Found completed task: {task_id}{RESET}")
            else:
                print(f"{BLUE}Skipping file {task_file.name} - not a valid task result{RESET}")
                
        except Exception as e:
            print(f"{RED}Warning: Could not read {task_file.name}: {e}{RESET}")
    
    print(f"{BLUE}Found {len(completed_tasks)} completed tasks in existing output folder{RESET}")
    
    return True, completed_tasks, params_data

def filter_tasks_for_continuation(selected_tasks, completed_tasks):
    """
    Filter the selected tasks to only include those not yet completed.
    
    Args:
        selected_tasks (list): List of (task_id, task_data) tuples
        completed_tasks (set): Set of task IDs that have been completed
        
    Returns:
        tuple: (remaining_tasks, skipped_count)
               remaining_tasks: List of (task_id, task_data) tuples for incomplete tasks
               skipped_count: Number of tasks skipped because they were already completed
    """
    remaining_tasks = []
    skipped_count = 0
    
    for task_id, task_data in selected_tasks:
        if task_id in completed_tasks:
            print(f"{BLUE}Skipping already completed task: {task_id}{RESET}")
            skipped_count += 1
        else:
            remaining_tasks.append((task_id, task_data))
    
    print(f"{GREEN}Tasks to process: {len(remaining_tasks)}{RESET}")
    print(f"{BLUE}Tasks already completed: {skipped_count}{RESET}")
    
    return remaining_tasks, skipped_count

# ============================================================================
# CORE LOGIC
# ============================================================================


def main():
    # Parse command line arguments - they override the ALL_CAPS defaults
    args = parse_arguments()
    
    # Use argparse values (which default to ALL_CAPS variables)
    task_index = args.task_index
    # Convert task_index to appropriate type (int, str, or None)
    if task_index is not None:
        if isinstance(task_index, str):
            if task_index.lower() == 'none':
                task_index = None
            elif task_index.isdigit() and len(task_index) <= 4:
                # Only convert to int if it's a reasonable index (<=4 digits)
                # ARC task IDs are typically 8 character hex strings
                task_index = int(task_index)
            # else keep as string (task ID)
    
    number_of_tasks = args.number_of_tasks
    model = args.model
    training_challenges_path = args.training_challenges_path
    training_solutions_path = args.training_solutions_path
    evaluation_challenges_path = args.evaluation_challenges_path
    evaluation_solutions_path = args.evaluation_solutions_path
    random_seed = args.random_seed
    temperature = args.temperature
    num_predict = args.num_predict
    sanitize_task = args.sanitize_task
    sanitize_id = args.sanitize_id
    parallel = args.parallel
    print_input = args.print_input
    print_output = args.print_output
    print_json = args.print_json
    use_smart_router = args.use_smart_router
    max_reflections = args.max_reflections
    task_retries = args.task_retries
    existing_output = args.existing_output
    
    # Load environment variables from .env file
    load_dotenv()
    print("Loaded environment variables from .env file")
    
    # Handle output directory - either create new or use existing
    completed_tasks = set()
    previous_params = None
    
    if existing_output:
        # Check if existing output folder is valid
        is_valid, completed_tasks, previous_params = check_existing_output_folder(existing_output)
        
        if is_valid:
            output_dir = Path(existing_output)
            print(f"{GREEN}Continuing run from existing output folder: {output_dir.absolute()}{RESET}")
            
            # Display previous run parameters if available
            if previous_params:
                print(f"{BLUE}Previous run parameters:{RESET}")
                for key, value in previous_params.items():
                    print(f"  {key}: {value}")
        else:
            print(f"{RED}Invalid existing output folder. Creating new run instead.{RESET}")
            existing_output = None  # Fall back to creating new folder
    
    if not existing_output:
        # Create output directory with timestamp
        timestamp = datetime.now().isoformat().replace(':', '-').replace('.', '-')
        output_dir = Path("output") / timestamp
        output_dir.mkdir(parents=True, exist_ok=True)
        print(f"Created output directory: {output_dir}")
        print(f"Results will be saved to: {output_dir.absolute()}")
    
    print(f"Output directory: {output_dir.absolute()}")
    
    # Save parameters to params.json (update existing or create new)
    params = {
        'script': 'test_arc_prompt.py',
        'task_index': task_index,
        'number_of_tasks': number_of_tasks,
        'model': model,
        'training_challenges_path': training_challenges_path,
        'training_solutions_path': training_solutions_path,
        'evaluation_challenges_path': evaluation_challenges_path,
        'evaluation_solutions_path': evaluation_solutions_path,
        'random_seed': random_seed,
        'temperature': temperature,
        'num_predict': num_predict,
        'sanitize_task': sanitize_task,
        'sanitize_id': sanitize_id,
        'parallel': parallel,
        'print_input': print_input,
        'print_output': print_output,
        'use_smart_router': use_smart_router,
        'max_reflections': max_reflections,
        'task_retries': task_retries,
        'existing_output': existing_output,
        'timestamp': datetime.now().isoformat().replace(':', '-').replace('.', '-'),
        'continuation_run': existing_output is not None
    }
    
    params_file = output_dir / 'params.json'
    with open(params_file, 'w') as f:
        json.dump(params, f, indent=2)
    
    if existing_output:
        print(f"Parameters updated in {params_file}")
    else:
        print(f"Parameters saved to {params_file}")
    
    
    # Debug: Check if API key is loaded
    gemini_key = os.getenv('GEMINI_API_KEY')
    if gemini_key:
        print(f"âœ“ GEMINI_API_KEY loaded (first 10 chars: {gemini_key[:10]}...)")
    else:
        print("âœ— GEMINI_API_KEY not found in environment")
    
    # Set random seed if provided
    if random_seed is not None:
        random.seed(random_seed)
        print(f"Using random seed: {random_seed}")
    
    # Check if model is known
    if not is_known_model(model):
        print(f"Error: Unknown model '{model}'")
        print(f"Available models: {', '.join(MODEL_CONFIGS.keys())}")
        sys.exit(1)
    
    # Load ARC challenges
    training_challenges_path = Path(training_challenges_path)
    if not training_challenges_path.exists():
        print(f"Error: Training challenges file not found: {training_challenges_path}")
        sys.exit(1)
    
    evaluation_challenges_path = Path(evaluation_challenges_path)
    if not evaluation_challenges_path.exists():
        print(f"Error: Evaluation challenges file not found: {evaluation_challenges_path}")
        sys.exit(1)

    try:
        training_tasks = load_arc_tasks(training_challenges_path)
        print(f"Loaded {len(training_tasks)} ARC training tasks from {training_challenges_path}")
        evaluation_tasks = load_arc_tasks(evaluation_challenges_path)
        print(f"Loaded {len(evaluation_tasks)} ARC evaluation tasks from {evaluation_challenges_path}")
        
        # Combine tasks for processing
        tasks = {**training_tasks, **evaluation_tasks}
        print(f"Total tasks available: {len(tasks)}")
    except Exception as e:
        print(f"Error loading tasks: {e}")
        sys.exit(1)
    
    # Load ARC solutions
    training_solutions_path = Path(training_solutions_path)
    if not training_solutions_path.exists():
        print(f"Error: Training solutions file not found: {training_solutions_path}")
        sys.exit(1)
        
    evaluation_solutions_path = Path(evaluation_solutions_path)
    if not evaluation_solutions_path.exists():
        print(f"Error: Evaluation solutions file not found: {evaluation_solutions_path}")
        sys.exit(1)

    try:
        training_solutions = load_arc_solutions(training_solutions_path)
        print(f"Loaded training solutions for {len(training_solutions)} tasks from {training_solutions_path}")
        evaluation_solutions = load_arc_solutions(evaluation_solutions_path)
        print(f"Loaded evaluation solutions for {len(evaluation_solutions)} tasks from {evaluation_solutions_path}")
        
        # Combine solutions for processing
        solutions = {**training_solutions, **evaluation_solutions}
        print(f"Total solutions available: {len(solutions)}")
    except Exception as e:
        print(f"Error loading solutions: {e}")
        sys.exit(1)
    
    # Get model configuration
    model_config = MODEL_CONFIGS.get(model, {})
    provider = model_config.get('provider', 'unknown')
    
    # Select tasks to run
    try:
        selected_tasks = get_multiple_tasks(tasks, number_of_tasks, task_index)
    except ValueError as e:
        print(f"Error: {e}")
        sys.exit(1)
    
    # Filter tasks if continuing an existing run
    original_task_count = len(selected_tasks)
    if existing_output and completed_tasks:
        selected_tasks, skipped_count = filter_tasks_for_continuation(selected_tasks, completed_tasks)
        
        if not selected_tasks:
            print(f"\n{GREEN}All tasks in this run have already been completed!{RESET}")
            print(f"Total tasks: {original_task_count}, Completed: {skipped_count}")
            print(f"Output directory: {output_dir.absolute()}")
            
            # Still show summary if available
            try:
                test_summary(str(output_dir))
            except Exception as e:
                print(f"Error generating summary: {e}")
            
            print(f"\n{'='*60}")
            print("RUN COMPLETED - ALL TASKS ALREADY DONE")
            print(f"{'='*60}")
            return
        
        print(f"\n{BLUE}CONTINUATION RUN SUMMARY:{RESET}")
        print(f"  Original tasks selected: {original_task_count}")
        print(f"  Already completed: {skipped_count}")
        print(f"  Remaining to process: {len(selected_tasks)}")
    
    print(f"\n{'='*100}")
    print(f"RUNNING {len(selected_tasks)} TASK(S) WITH MODEL: {model}")
    if existing_output:
        print(f"CONTINUING FROM: {output_dir.absolute()}")
    print(f"{'='*100}")
    
    # Process all tasks
    all_results = []
    total_tokens = 0
    total_estimated_cost = 0.0
    successful_json_generations = 0
    
    print(f"\n{BLUE}{'='*100}")
    print(f"PARALLEL EXECUTION DEBUG INFO:")
    print(f"  - PARALLEL setting: {parallel}")
    print(f"  - Selected tasks count: {len(selected_tasks)}")
    print(f"  - Model: {model}")
    print(f"  - Provider: {provider}")
    print(f"{'='*100}{RESET}")

    # Check if we should disable parallelization for local models
    if parallel and len(selected_tasks) > 1:
        # Disable parallel processing for local models (ollama, qwen, etc.)
        if provider == "ollama" or model in ["llama3.1", "qwen2.5:32b"] or "qwen" in model.lower() or "llama" in model.lower():
            print(f"\n{BLUE}Disabling parallel processing for local model: {model}{RESET}")
            print(f"{BLUE}Local models work better with sequential processing{RESET}")
            parallel = False
    
    if parallel and len(selected_tasks) > 1:
        print(f"\n{BLUE}Running {len(selected_tasks)} tasks in parallel...{RESET}")
        print(f"{BLUE}Output for each task will be displayed as it completes{RESET}")
        
        # Prepare arguments for parallel execution
        task_args = [(task_id, task_data, solutions, model, provider, output_dir, sanitize_task, sanitize_id, print_input, print_output, print_json, use_smart_router, max_reflections, task_retries) 
                    for task_id, task_data in selected_tasks]
        
        # Create a lock for synchronized printing
        print_lock = threading.Lock()
        
        # Get max concurrency from environment variable, default to 4
        max_concurrency = int(os.getenv('MAX_CONCURRENCY', '4'))
        max_workers = min(len(selected_tasks), max_concurrency)
        
        print(f"{BLUE}Using max_workers={max_workers} (MAX_CONCURRENCY={max_concurrency}){RESET}")
        
        # Execute tasks in parallel
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all tasks
            print(f"{BLUE}Submitting {len(task_args)} tasks to executor...{RESET}")
            future_to_task = {}
            
            for i, args in enumerate(task_args):
                future = executor.submit(run_single_task_wrapper, args)
                future_to_task[future] = args[0]  # Store task_id
                print(f"{BLUE}  Submitted task {i+1}/{len(task_args)}: {args[0]}{RESET}")
            
            print(f"{BLUE}All {len(future_to_task)} tasks submitted to executor. Waiting for completion...{RESET}")
            sys.stdout.flush()
            
            # Collect results as they complete
            start_time = time.time()
            completed_count = 0
            for future in as_completed(future_to_task):
                completed_count += 1
                current_time = time.time()
                elapsed = current_time - start_time
                
                print(f"\n{BLUE}â±ï¸ Task {completed_count}/{len(selected_tasks)} completed after {elapsed:.1f}s, processing result...{RESET}")
                sys.stdout.flush()
                sys.stderr.flush()
                
                task_id = future_to_task[future]
                try:
                    result, captured_output, task_id = future.result()
                    all_results.append(result)
                    
                    total_tokens += result['total_tokens']
                    total_estimated_cost += result['estimated_cost']
                    if result['transformations_json_generated']:
                        successful_json_generations += 1
                    
                    # Print the captured output for this task in a synchronized way
                    with print_lock:
                        status = "âœ“" if result['transformations_json_generated'] else "âœ—"
                        print(f"\n{GREEN}{'='*100}")
                        print(f"TASK COMPLETED: {task_id} ({completed_count}/{len(selected_tasks)}) {status}")
                        print(f"Tokens: {result['total_tokens']:,}, Cost: ${result['estimated_cost']:.6f}")
                        print(f"{'='*100}{RESET}")
                        print(captured_output)  # Print all the captured output
                        print(f"{GREEN}{'='*100}")
                        print(f"END OF TASK: {task_id}")
                        print(f"{'='*100}{RESET}")
                        # Force flush to make sure output appears immediately
                        sys.stdout.flush()
                        sys.stderr.flush()
                    
                except Exception as e:
                    with print_lock:
                        print(f"{RED}âœ— Error processing {task_id}: {e}{RESET}")
                        sys.stdout.flush()
                        sys.stderr.flush()
                    # Create empty result for failed task
                    error_result = {
                        'total_tokens': 0,
                        'input_tokens': 0,
                        'output_tokens': 0,
                        'estimated_cost': 0.0,
                        'transformations_json_generated': False,
                        'transformations_json': None,
                        'trains': [],
                        'tests': []
                    }
                    all_results.append(error_result)
        
        # Sort results by processing order (we don't have task_id in new format)
        # Results are already in completion order which is fine
        
    elif parallel and len(selected_tasks) == 1:
        # Special case: parallel enabled but only 1 task - use buffering anyway
        print(f"\n{BLUE}Running 1 task with output buffering...{RESET}")
        task_id, task_data = selected_tasks[0]
        
        with OutputCapture() as captured_output:
            result = run_task_with_retries(task_id, task_data, solutions, model, provider, output_dir, 
                                          sanitize_task, sanitize_id, print_input, print_output, 
                                          print_json, use_smart_router, max_reflections, task_retries)
        
        # Print the captured output
        status = "âœ“" if result['transformations_json_generated'] else "âœ—"
        print(f"\n{GREEN}{'='*100}")
        print(f"TASK COMPLETED: {task_id} (1/1) {status}")
        print(f"{'='*100}{RESET}")
        print(captured_output.getvalue())
        print(f"{GREEN}{'='*100}")
        print(f"END OF TASK: {task_id}")
        print(f"{'='*100}{RESET}")
        
        all_results.append(result)
        total_tokens += result['total_tokens']
        total_estimated_cost += result['estimated_cost']
        if result['transformations_json_generated']:
            successful_json_generations += 1
            
    else:
        # Sequential processing with reflection-based retries
        task_results = {}  # Store results by task_id
        completed_tasks = set()  # Track which tasks are fully completed
        task_runs_used = 0  # Track total runs used
        total_task_runs = args.total_task_runs  # Get from arguments
        
        # Phase 1: Initial run of all tasks
        print(f"\n{'='*100}")
        print(f"PHASE 1: INITIAL RUN OF {len(selected_tasks)} TASKS")
        print(f"Total task runs available: {total_task_runs}")
        print(f"{'='*100}")
        
        for i, (task_id, task_data) in enumerate(selected_tasks):
            if task_runs_used >= total_task_runs:
                print(f"\n{RED}Reached total task runs limit ({total_task_runs}). Stopping.{RESET}")
                break
                
            print(f"\n{'='*100}")
            print(f"INITIAL ATTEMPT - TASK {i+1}/{len(selected_tasks)}: {task_id}")
            print(f"Task runs used: {task_runs_used + 1}/{total_task_runs}")
            print(f"{'='*100}")
            
            result = run_task_with_retries(task_id, task_data, solutions, model, provider, output_dir, 
                                          sanitize_task, sanitize_id, print_input, print_output, 
                                          print_json, use_smart_router, max_reflections, task_retries)
            
            task_results[task_id] = result
            task_runs_used += 1
            
            # Check if task completed successfully (all training examples correct)
            if is_task_completed_successfully(result):
                completed_tasks.add(task_id)
                print(f"{GREEN}âœ“ Task {task_id} completed successfully on initial attempt!{RESET}")
            else:
                print(f"{RED}âœ— Task {task_id} failed on initial attempt. Will retry with reflection.{RESET}")
            
            total_tokens += result['total_tokens']
            total_estimated_cost += result['estimated_cost']
            if result['transformations_json_generated']:
                successful_json_generations += 1
        
        # Phase 2: Reflection-based retries for failed tasks
        failed_tasks = [(task_id, task_data) for task_id, task_data in selected_tasks 
                       if task_id in task_results and task_id not in completed_tasks]
        
        reflection_round = 1
        while failed_tasks and task_runs_used < total_task_runs:
            print(f"\n{'='*100}")
            print(f"PHASE 2: REFLECTION ROUND {reflection_round}")
            print(f"Failed tasks to retry: {len(failed_tasks)}")
            print(f"Task runs used: {task_runs_used}/{total_task_runs}")
            print(f"{'='*100}")
            
            current_failed_tasks = failed_tasks.copy()
            failed_tasks = []  # Reset for next round
            
            for i, (task_id, task_data) in enumerate(current_failed_tasks):
                if task_runs_used >= total_task_runs:
                    print(f"\n{RED}Reached total task runs limit ({total_task_runs}). Stopping retries.{RESET}")
                    # Add remaining tasks back to failed list
                    failed_tasks.extend(current_failed_tasks[i:])
                    break
                
                print(f"\n{'='*80}")
                print(f"REFLECTION ATTEMPT {reflection_round} - TASK {i+1}/{len(current_failed_tasks)}: {task_id}")
                print(f"Task runs used: {task_runs_used + 1}/{total_task_runs}")
                print(f"{'='*80}")
                
                # Get previous result for reflection
                previous_result = task_results[task_id]
                
                # Run with reflection prompt
                result = run_task_with_reflection(task_id, task_data, solutions, model, provider, 
                                                previous_result, output_dir, sanitize_task, sanitize_id, 
                                                print_input, print_output, print_json, use_smart_router, 
                                                max_reflections, task_retries)
                
                # Update result and counters
                if result:
                    task_results[task_id] = result
                    task_runs_used += 1
                    
                    # Check if task completed successfully now
                    if is_task_completed_successfully(result):
                        completed_tasks.add(task_id)
                        print(f"{GREEN}âœ“ Task {task_id} completed successfully after reflection round {reflection_round}!{RESET}")
                    else:
                        failed_tasks.append((task_id, task_data))  # Still failed, try again next round
                        print(f"{RED}âœ— Task {task_id} still failed after reflection round {reflection_round}.{RESET}")
                    
                    total_tokens += result['total_tokens']
                    total_estimated_cost += result['estimated_cost']
                    if result['transformations_json_generated']:
                        successful_json_generations += 1
                else:
                    print(f"{RED}âœ— Failed to get result for task {task_id} in reflection round {reflection_round}{RESET}")
                    failed_tasks.append((task_id, task_data))  # Keep in failed list
            
            reflection_round += 1
        
        # Prepare final results list
        for task_id, task_data in selected_tasks:
            if task_id in task_results:
                all_results.append(task_results[task_id])
        
        # Print completion summary
        print(f"\n{'='*100}")
        print("TASK COMPLETION SUMMARY")
        print(f"{'='*100}")
        print(f"Total tasks: {len(selected_tasks)}")
        print(f"Successfully completed: {len(completed_tasks)}")
        print(f"Still failed: {len(selected_tasks) - len(completed_tasks)}")
        print(f"Total task runs used: {task_runs_used}/{total_task_runs}")
        print(f"Reflection rounds completed: {reflection_round - 1}")
        
        if failed_tasks:
            print(f"\n{RED}Tasks that remained failed after all retries:{RESET}")
            for task_id, _ in failed_tasks:
                print(f"  - {task_id}")
        
        if completed_tasks:
            print(f"\n{GREEN}Tasks completed successfully:{RESET}")
            for task_id in completed_tasks:
                print(f"  - {task_id}")
    
    # Print final statistics
    print(f"\n{'='*100}")
    print("FINAL RESULTS SUMMARY")
    print(f"{'='*100}")
    
    # Calculate statistics from new format
    total_correct_tests = 0
    total_correct_trains = 0
    total_test_count = 0
    total_train_count = 0
    avg_test_overlap = 0.0
    avg_train_overlap = 0.0
    
    for result in all_results:
        # Count test results
        for test_result in result.get('tests', []):
            total_test_count += 1
            if test_result.get('correct', False):
                total_correct_tests += 1
            avg_test_overlap += test_result.get('overlap', 0.0)
        
        # Count train results
        for train_result in result.get('trains', []):
            total_train_count += 1
            if train_result.get('correct', False):
                total_correct_trains += 1
            avg_train_overlap += train_result.get('overlap', 0.0)
    
    # Calculate averages
    if total_test_count > 0:
        avg_test_overlap /= total_test_count
        test_accuracy = (total_correct_tests / total_test_count) * 100
    else:
        test_accuracy = 0.0
        avg_test_overlap = 0.0
    
    if total_train_count > 0:
        avg_train_overlap /= total_train_count
        train_accuracy = (total_correct_trains / total_train_count) * 100
    else:
        train_accuracy = 0.0
        avg_train_overlap = 0.0

    print(f"\n{BLUE}TASK PERFORMANCE:{RESET}")
    print(f"  Total tasks processed: {len(selected_tasks)}")
    print(f"  Successful JSON generations: {successful_json_generations}/{len(selected_tasks)} ({successful_json_generations/len(selected_tasks)*100:.1f}%)")
    print(f"  Total tokens used: {total_tokens:,}")
    print(f"  Total estimated cost: ${total_estimated_cost:.6f}")
    print(f"  Average cost per task: ${total_estimated_cost/len(selected_tasks):.6f}")
    
    print(f"\n{BLUE}TEST PERFORMANCE:{RESET}")
    print(f"  Total test examples: {total_test_count}")
    print(f"  Correct test predictions: {total_correct_tests}")
    print(f"  Test accuracy: {test_accuracy:.1f}%")
    print(f"  Average test overlap: {avg_test_overlap:.1f}%")
    
    print(f"\n{BLUE}TRAINING PERFORMANCE:{RESET}")
    print(f"  Total training examples: {total_train_count}")
    print(f"  Correct training predictions: {total_correct_trains}")
    print(f"  Training accuracy: {train_accuracy:.1f}%")
    print(f"  Average training overlap: {avg_train_overlap:.1f}%")
    
    print(f"\n{BLUE}DETAILED BREAKDOWN:{RESET}")
    for i, result in enumerate(all_results):
        task_id = selected_tasks[i][0] if i < len(selected_tasks) else f"task_{i}"
        json_status = "âœ“" if result['transformations_json_generated'] else "âœ—"
        cost = result['estimated_cost']
        tokens = result['total_tokens']
        print(f"  {json_status} {task_id} - Tokens: {tokens:,}, Cost: ${cost:.6f}")
    
    # Overall performance message
    if successful_json_generations == len(selected_tasks):
        print(f"\n{GREEN}ðŸŽ‰ ALL TASKS GENERATED JSON! {successful_json_generations}/{len(selected_tasks)} successful generations.{RESET}")
    elif successful_json_generations > len(selected_tasks) * 0.7:
        print(f"\n{GREEN}ðŸŸ¢ EXCELLENT PERFORMANCE! {successful_json_generations}/{len(selected_tasks)} successful JSON generations.{RESET}")
    elif successful_json_generations > len(selected_tasks) * 0.3:
        print(f"\n{BLUE}ðŸŸ¡ MODERATE PERFORMANCE. {successful_json_generations}/{len(selected_tasks)} successful JSON generations.{RESET}")
    else:
        print(f"\n{RED}ðŸ”´ POOR PERFORMANCE. Only {successful_json_generations}/{len(selected_tasks)} successful JSON generations.{RESET}")
    
    # Generate summary statistics
    print(f"\nGenerating summary statistics...")
    try:
        calculate_results(str(output_dir))
    except Exception as e:
        print(f"Error generating summary: {e}")
    
    # Generate task ID files
    print(f"\nGenerating task ID files...")
    try:
        training_task_ids = list(training_tasks.keys())
        evaluation_task_ids = list(evaluation_tasks.keys())
        
        # Write training task IDs
        training_ids_file = output_dir / "training_task_ids.txt"
        with open(training_ids_file, 'w') as f:
            f.write('\n'.join(training_task_ids))
        print(f"Training task IDs saved to: {training_ids_file}")
        
        # Write evaluation task IDs
        evaluation_ids_file = output_dir / "evaluation_task_ids.txt"
        with open(evaluation_ids_file, 'w') as f:
            f.write('\n'.join(evaluation_task_ids))
        print(f"Evaluation task IDs saved to: {evaluation_ids_file}")
        
    except Exception as e:
        print(f"Error generating task ID files: {e}")

    # Display detailed test summary
    print(f"\nDisplaying test summary...")
    try:
        test_summary(str(output_dir))
    except Exception as e:
        print(f"Error generating test summary: {e}")

    print(f"\n{'='*100}")
    print("RUN COMPLETED")
    print(f"{'='*100}")
    print(f"Results saved in: {output_dir.absolute()}")


if __name__ == "__main__":
    main()